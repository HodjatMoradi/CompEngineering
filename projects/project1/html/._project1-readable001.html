<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="description" content="MOD510: Mandatory project \#1">

<title>MOD510: Mandatory project \#1</title>

<!-- Bootstrap style: bootswatch_readable -->
<link href="https://netdna.bootstrapcdn.com/bootswatch/3.1.1/readable/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">
/* Let inline verbatim have the same color as the surroundings */
code { color: inherit; background-color: transparent; }

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:64px;      /* fixed header height for style bootswatch_readable */
  margin:-64px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 1,
 'sections': [('Table of contents',
               1,
               'table_of_contents',
               'table_of_contents'),
              ('Exercise 1: Visualize oil production data', 1, None, '___sec0'),
              ('Exercise 2: More wells, more oil?', 1, None, '___sec1'),
              ('Exercise 3: Finite differences', 1, None, '___sec2'),
              ('Exercise 4: Fitting a simple linear model to data',
               1,
               None,
               '___sec3'),
              ('Exercise 5: Error analysis for finite difference schemes',
               1,
               None,
               '___sec4'),
              ('Exercise 6: (Optional) Multiple linear regression',
               1,
               'linear_regression_normal_eqs',
               'linear_regression_normal_eqs'),
              ('Guidelines for project submission', 1, None, '___sec6')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "none"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="project1-readable.html">MOD510: Mandatory project \#1</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="._project1-readable000.html#table_of_contents" style="font-size: 80%;">Table of contents</a></li>
     <!-- navigation toc: --> <li><a href="#___sec0" style="font-size: 80%;">Exercise 1: Visualize oil production data</a></li>
     <!-- navigation toc: --> <li><a href="#___sec1" style="font-size: 80%;">Exercise 2: More wells, more oil?</a></li>
     <!-- navigation toc: --> <li><a href="#___sec2" style="font-size: 80%;">Exercise 3: Finite differences</a></li>
     <!-- navigation toc: --> <li><a href="#___sec3" style="font-size: 80%;">Exercise 4: Fitting a simple linear model to data</a></li>
     <!-- navigation toc: --> <li><a href="#___sec4" style="font-size: 80%;">Exercise 5: Error analysis for finite difference schemes</a></li>
     <!-- navigation toc: --> <li><a href="#linear_regression_normal_eqs" style="font-size: 80%;">Exercise 6: (Optional) Multiple linear regression</a></li>
     <!-- navigation toc: --> <li><a href="#___sec6" style="font-size: 80%;">Guidelines for project submission</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<a name="part0001"></a>
<!-- !split -->

<h1 id="___sec0" class="anchor">Exercise 1: Visualize oil production data </h1>

<p>
When analyzing data it is very advantageous to start by making a plot.
The human mind is usually good at detecting patterns, and by looking at
the data one can think of simple ideas to test out, before possibly
doing a more comprehensive analysis.

<p>
As part of this project we will look at some of the datasets that
are available at the Norwegian Petroleum Directorate (NPD)
<a href="http://factpages.npd.no/factpages/" target="_self">website</a>.
These data are updated regularly, and if you are able to make a good model
of, e.g., the historical oil production versus time, you could use it to
forecast the production in the future as well.

<p>
<b>Part 1.</b>

<p>
A challenge when doing data analysis is that the data is usually not available
in the format you would like it to be in. In Python there are many ways of
preprocessing and manipulating data, but the
<a href="https://pandas.pydata.org" target="_self"><tt>Pandas</tt></a> library is particularly well-suited
for the purpose.
Below is a code snippet to help you to get started with reading data from Excel
into a <code>Pandas</code> <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html" target="_self"><tt>DataFrame</tt></a>:

<p>

<!-- code=python (!bc pypro) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">numpy</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">np</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pandas</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">pd</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">pathlib</span>
<span style="color: #008000; font-weight: bold">import</span> <span style="color: #0000FF; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008000; font-weight: bold">as</span> <span style="color: #0000FF; font-weight: bold">plt</span>

<span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">df_field</span>(name, datafile<span style="color: #666666">=</span><span style="color: #BA2121">&#39;field_production_gross_monthly.xls&#39;</span>, col<span style="color: #666666">=0</span>):
    folder <span style="color: #666666">=</span> pathlib<span style="color: #666666">.</span>Path<span style="color: #666666">.</span>cwd()<span style="color: #666666">.</span>parent<span style="color: #666666">.</span>joinpath(<span style="color: #BA2121">&#39;data&#39;</span>)
    filename <span style="color: #666666">=</span> folder<span style="color: #666666">.</span>joinpath(datafile)
    df <span style="color: #666666">=</span> pd<span style="color: #666666">.</span>read_excel(filename)
    columns <span style="color: #666666">=</span> df<span style="color: #666666">.</span>columns
    <span style="color: #008000; font-weight: bold">return</span> df[df[df<span style="color: #666666">.</span>columns[col]] <span style="color: #666666">==</span> name]
</pre></div>
<p>
Note that all data files needed for this project are located in the <code>data</code>
folder.

<ul>
<li> Explain in your own words what the above Python function does.</li>
<li> Open the file <code>field_production_gross_monthly.xls</code>, and compare its contents with output from the command <code>print(df_field('OSEBERG'))</code>.</li>
</ul>

<b>Part 2.</b>

<p>
Next, we wish to look at the historical oil production versus time for a
specific field. Such plots are already available at the
<a href="http://factpages.npd.no/factpages/" target="_self">NPD website</a>, but here you are going to
make your own by using the <a href="https://matplotlib.org" target="_self"><tt>matplotlib</tt></a> library.

<p>
When we develop code, it is good practice to use functions to divide a
problem into smaller pieces. We start by extracting oil production data
for a specific field with the function already defined in
Part 1:

<p>

<!-- code=python (!bc pypro) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">prod_data</span>(name):
    df <span style="color: #666666">=</span> df_field(name)
    columns <span style="color: #666666">=</span> df<span style="color: #666666">.</span>columns
    Year    <span style="color: #666666">=</span> df[ <span style="color: #666666">...</span> ]
    Month   <span style="color: #666666">=</span> df[ <span style="color: #666666">...</span> ]
    OilProd <span style="color: #666666">=</span> df[ <span style="color: #666666">...</span> ]
    <span style="color: #408080; font-style: italic">#Assume 30 days in each month and 365 in year</span>
    Year    <span style="color: #666666">=</span> Year <span style="color: #666666">+</span> Month<span style="color: #666666">*30/365</span>
    <span style="color: #008000; font-weight: bold">return</span> Year, OilProd
</pre></div>
<ul>
<li> Complete writing the above function.</li>
</ul>

<b>Part 3.</b>

<ul>
<li> We also make a function that plots the oil production versus time:</li>
</ul>

<p>

<!-- code=python (!bc pypro) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_prod_data</span>(name):
    Year, OilProd<span style="color: #666666">=</span>prod_data(name)
    <span style="color: #666666">...</span>  <span style="color: #408080; font-style: italic"># make plot here</span>
</pre></div>
<ul>
<li> Finish implementing this Python function as well.</li>
</ul>

Calling <code>plot_prod_data('OSEBERG')</code> should generate a figure that looks
something like this:

<p>
<br /><br /><center><p><img src="fig-project1/oseberg.png" align="bottom" width=400></p></center><br /><br />

<p>
<b>Part 4 (Optional).</b>
In the oil production plot for a given field, we additionally want to include
information about the number of wells that have been drilled. Specifically,
we wish to plot, on a separate \( y \)-axis, the cumulative number of wells present
in the field at any given moment in time.

<p>
Well data is stored in the file <code>wellbore_development_all.xls</code>.
In this file, each row entry corresponds to a single well, and the
field to which it belongs is stored in the column with index 14.
Hence, we can start by writing:

<p>

<!-- code=python (!bc pypro) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">plot_prod_and_well_data</span>(name):
    fn <span style="color: #666666">=</span> <span style="color: #BA2121">&#39;wellbore_development_all.xls&#39;</span>
    df <span style="color: #666666">=</span> df_field(name, datafile<span style="color: #666666">=</span>fn, col<span style="color: #666666">=14</span>)
    columns <span style="color: #666666">=</span> df<span style="color: #666666">.</span>columns
    year <span style="color: #666666">=</span> df[<span style="color: #BA2121">&#39;Completed year&#39;</span>]  <span style="color: #408080; font-style: italic"># alternatively: fetch by index (32)</span>
    <span style="color: #666666">...</span>
</pre></div>
<p>
Note that the Excel file only tells you the year in which each well was
completed; <em>there could be many wells drilled in a given year, and the list is
not sorted</em>. This means that you need to a little more work to process the
data:

<ul>
<li> Finish implementing the above Python function.</li>
<li> Calling it with a specific field as input should generate a plot that looks something like this:</li>
</ul>

<br /><br /><center><p><img src="fig-project1/oseberg2.png" align="bottom" width=800></p></center><br /><br />

<p>
If you want, you can also distinguish between different types of wells, i.e.,
between injection/production/observation wells.

<h1 id="___sec1" class="anchor">Exercise 2: More wells, more oil? </h1>

<p>
A very common statement is "more wells - more oil". The fields on the Norwegian
Continental Shelf (NCS) are in different stages of production, and they have
varying reservoir properties, so a thorough analysis would take time.
However, we would at least expect to see some correlation between the
<em>total amount of oil produced</em> and <em>the number of wells drilled</em>, if the
hypothesis is correct.

<p>
<b>Part 1.</b>
The first task is to fetch the (final) number of wells drilled for each field,
as well as the total (cumulative) oil production. The resulting values are
to be stored in arrays:

<ul>
<li> The hard way is to do this yourself by extracting the data you need from a combination of the provided NPD <code>.xls</code> files.</li>
<li> Alternatively, you can 'cheat' and use the preprocessed data stored in <code>fields_oil_wells.xlsx</code>.</li>
</ul>

<b>Part 2.</b>

<ul>
<li> Make a scatter plot showing the cumulative oil production of all the fields on the \( y \)-axis. On the \( x \)-axis, plot the number of wells for the fields (you might want to exclude observation wells).</li>
</ul>

(We will return to this data set later in this project, after introducing linear regression.)

<p>
<b>Part 3.</b>
The Draugen field is very homogeneous, and it is therefore regarded as an ideal
field on the NCS. It also has an active aquifer underlying the reservoir,
meaning that when producing oil by water injection the aquifer provides
additional pressure support.

<ul>
<li> Does the plot you made in Part 2 indicate that Draugen has been a successful field compared to the others? Why/why not?</li>
</ul>

<h1 id="___sec2" class="anchor">Exercise 3: Finite differences </h1>

<p>
The most straightforward way to approximate the derivative of a function
\( f=f(x) \) is to use the function value at \( x \) <em>and</em> at a small distance \( h \)
from \( x \), e.g.:
$$
\begin{equation}
f^{\prime}(x)\approx\frac{f(x+h)-f(x)}{h}\,.
\tag{1}
\end{equation}
$$

This approximation is called the <em>forward difference</em>.
For 'well-behaved' functions, it can be shown using Taylor's formula
that
$$
\begin{equation}
\tag{2}
f^{\prime}(x)=\frac{f(x+h)-f(x)}{h}-h\frac{f^{\prime\prime}(\xi)}{2}\,,
\end{equation}
$$

for some \( \xi \) between \( x \) and \( x+h \).

<p>
<b>Part 1.</b>
Explain in detail how equation <a href="#mjx-eqn-2">(2)</a> can be derived from Taylor's
formula.

<p>
What is the order of the truncation error for the forward difference numerical
differentiation method? What should ideally happen if you lower the step size
by a factor of ten?

<p>
<b>Part 2.</b>
Alternatively, we could use the <em>backward difference</em> approximation:
$$
\begin{equation}
f^{\prime}(x)\approx\frac{f(x)-f(x-h)}{h}\,.
\tag{3}
\end{equation}
$$

For this method, use Taylor expansions to derive a similar expression
as <a href="#mjx-eqn-2">(2)</a>.

<p>
What is the truncation error for this method?

<p>
<b>Part 3.</b>
The <em>central difference</em> approximation of the derivative is given by
$$
\begin{equation}
f^{\prime}(x)\approx\frac{f(x+h)-f(x-h)}{2h}\,.
\tag{4}
\end{equation}
$$

Again, use Taylor expansions to derive this expression. What is the truncation
error, and how does it compare to the other two methods?
(hint: combine the derivations you did for the forward and backward
finite difference schemes)

<p>
<b>Part 4.</b>
Implement a Python function that can calculate numerical derivatives of
functions \( f=f(x) \). The routine should be able to handle all of the three
aforementioned formulas (forward difference, backward difference, central
difference), as well as different values of the step size, \( h \).

<p>
You may take the following code as a starting point:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">numerical_derivative</span>(f, x, <span style="color: #666666">*</span>, method<span style="color: #666666">=</span><span style="color: #BA2121">&#39;forward&#39;</span>, h<span style="color: #666666">=1.0e-4</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Function to numerically evaluate the derivative of a function</span>
<span style="color: #BA2121; font-style: italic">    f=f(x).</span>

<span style="color: #BA2121; font-style: italic">    :param f: Function to differentiate.</span>
<span style="color: #BA2121; font-style: italic">    :param x: Point at which to evaluate the derivative.</span>
<span style="color: #BA2121; font-style: italic">    :param method: Approximation formula to apply. Available options</span>
<span style="color: #BA2121; font-style: italic">                   are &#39;forward&#39; (default), &#39;backward&#39;, and &#39;central&#39;</span>
<span style="color: #BA2121; font-style: italic">                   finite differences.</span>
<span style="color: #BA2121; font-style: italic">    :param h: Step size (default: 1.0e-4).</span>
<span style="color: #BA2121; font-style: italic">    :return: Approximate value for f&#39;(x).</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
</pre></div>
<p>
<b>Part 5.</b>

<p>
Consider the function
$$
f(x)=\sqrt{x^2+5}\,.
$$

<p>
We want to estimate the numerical error when calculating \( f^\prime(1) \) with
a finite difference formula:

<ul>
<li> First, show that the exact value of the derivative is \( f^\prime(1)=\frac{1}{\sqrt{6}} \).</li>
<li> Apply your implemented Python routine to numerically evaluate \( f^{\prime}(1) \). Do this for \( h=10^{-16}, h=10^{-15}, \ldots \), up to \( h=10^{-1} \), and for all of the three introduced finite difference formulas.</li>
<li> For each differentiation method, make a figure showing the absolute value of the error versus \( h \) on a double-logarithmic plot.</li>
</ul>

<h1 id="___sec3" class="anchor">Exercise 4: Fitting a simple linear model to data </h1>

<p>
When dealing with physical data points, they rarely fit a smooth curve, but
we would still like to calculate derivatives to report, e.g., how
fast a quantity is changing as a function of time. One way of achieving this
is to calculate the derivative directly from the data, using a numerical
differentiation formula. However, such an approach can be very sensitive to
the choice of points at which we evaluate the derivative.

<p>
Another approach is to fit a straight line to the data, and report the slope
of the line. Suppose we are given \( N \) data points (\( x_i \), \( y_i \)), and that
we wish to approximate \( y \) (the dependent variable) as a linear function of
\( x \) (the independent variable), i.e.:
$$
y\approx \alpha+\beta\cdot{x}\,.
$$

One common algorithm for doing this is the method of
<a href="https://en.wikipedia.org/wiki/Least_squares" target="_self">least squares</a>.
The idea here is to select the slope \( \beta \) and the intercept \( \alpha \) in
such a way that the sum of squares of distances from the line to the data
points are minimized. Mathematically, we need to make the following
expression as small as possible:

$$
\begin{equation}
\tag{5}
\chi^2=\displaystyle\sum_{i=1}^{N}\left(y_i-\alpha-\beta\cdot{x_i}\right)^2\,.
\end{equation}
$$

<p>
To find the minimum, we can differentiate <a href="#mjx-eqn-5">(5)</a> with respect
to both \( \alpha \) and \( \beta \), and set the resulting expressions equal to zero.
If we do that, it is possible to show that we must have
$$
\begin{equation}
\tag{6}
\alpha=\overline{y}-\beta\overline{x}\,,
\end{equation}
$$

and
$$
\begin{equation}
\tag{7}
\beta=\frac{\displaystyle\sum_{i=1}^{N}(x_i-\overline{x})(y_i-\overline{y})}
{\displaystyle\sum_{i=1}^{N}(x_i-\overline{x})^2}
\end{equation}
$$

in which \( \overline{x} \) and \( \overline{y} \) are arithmetic averages of
the \( x- \) and \( y \)-values:
$$
\begin{align}
\overline{x} &= \frac{1}{N}\displaystyle\sum_{i=1}^{N} x_{i} 
\tag{8}\\ 
\overline{y} &= \frac{1}{N}\displaystyle\sum_{i=1}^{N} y_{i} \,.
\tag{9}
\end{align}
$$

The algorithm is also referred to as <em>ordinary least squares
regression</em> (OLSR), or <em>simple linear regression</em>. It is a standard
technique used in statistical analyses of empirical data.
Often, the goal is to be able to predict the outcome \( y \) based on a
knowledge of \( x \).

<p>
<b>Part 1.</b>

<ul>
<li> Finish the implementation of the following Python function:</li>
</ul>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">OLSR</span>(x, y):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Custom implementation of Ordinary Least Squares Regression.</span>

<span style="color: #BA2121; font-style: italic">    :param x: Input data points for the independent variable (array)</span>
<span style="color: #BA2121; font-style: italic">    :param y: Input data points for the dependent variable (array)</span>
<span style="color: #BA2121; font-style: italic">    :return: Tuple consisting of (intercept, slope) of the regression line.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
</pre></div>
<p>
<b>Part 2.</b>

<ul>
<li> Test the correctness of your implementation by applying the function to a fabricated data set of your own choosing in which the relationship between \( x \) and \( y \) is exactly linear.</li>
</ul>

<b>Part 3.</b>

<p>
The goodness of fit of a statistical model is frequently evaluated by
considering the
<a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" target="_self">coefficient of determination</a>,
or \( R^2 \) (<em>R-squared</em>). The purpose of \( R^2 \) is to measure the proportion of
variation in the dependent variable that can be explained by variation in the
independent variable(s). For the cases investigated in this project, the
relevant formula is
$$
\begin{equation}
R^2=1-\frac{\displaystyle\sum_{i=1}^{N}(y_i-\hat{y_i})^2}
{\displaystyle\sum_{i=1}^{N}(y_i-\overline{y})^2}\,,
\tag{10}
\end{equation}
$$

in which \( \hat{y_i} \) is the prediction of the model for data point \( i \).
For least squares regression models it can be shown that \( R^2=\rho^2 \),
where \( \rho \) is
<a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" target="_self">Pearson's correlation coefficient</a>.

<ul>
<li> Fill in the missing details of the following Python function:</li>
</ul>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">calc_r_squared</span>(y_obs, y_pred):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Calculates R^2 for a linear regression model.</span>

<span style="color: #BA2121; font-style: italic">    :param y_obs: Array of observed values.</span>
<span style="color: #BA2121; font-style: italic">    :param y_pred: Array of corresponding predicted / calculated values.</span>
<span style="color: #BA2121; font-style: italic">    :return: Coefficient of deterermination (R^2) for the fit.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
</pre></div>
<p>
<b>Part 4.</b>
We will now investigate the scatter plot you made in Part 2
of Exercise 2 in a little more detail:

<ul>
<li> Extend the plot by including in it a) the best fit straight line to the data in the least squares sense, and b) Pearson's correlation coefficient for the fit.</li>
<li> Do any of the fields seem to stand out clearly from the rest?</li>
</ul>

<b>Part 5.</b>

<ul>
<li> Calculate the slope and intercept of the least squares line again, but this time use available function(s) from established Python libraries. Check that you get the same regression line as when using your own implementation.</li>
</ul>

As an example, you might opt for the
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_self"><tt>LinearRegression</tt></a>
class from the
<a href="https://scikit-learn.org/" target="_self"><tt>scikitlearn</tt></a>, library or perhaps the polynomial
interpolation function <code>polyfit</code> available in
<a href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyfit.html" target="_self"><tt>NumPy</tt></a>.
Whatever you choose, make sure that you read the documentation and explain in
your own words what you do as you go along.

<h1 id="___sec4" class="anchor">Exercise 5: Error analysis for finite difference schemes </h1>

<p>
<b>Part 1.</b>
Consider again the error plots you made for the three finite difference
formulas in Part 5 of Exercise 3.
In each case, fit a straight line to the right part of the error curve.
How do the resulting slopes compare to what you would expect from a
theoretical truncation error analysis?

<p>
Also fit a straight line to the left part of each error curve.

<p>
<b>Part 2.</b>
By taking absolute values, it follows from <a href="#mjx-eqn-2">(2)</a> that
the truncation error \( E(f;x_0;h) \) of the forward difference approximation
to the derivative at \( x=x_0 \) must be bounded above by
$$
\begin{equation}
\tag{11}
E(f;x_0;h)\leq\frac{h}{2}\cdot\text{max}_{x\in[x_0,x_0+h]}|f^{\prime\prime}(x)|\,.
\end{equation}
$$

For the function \( f(x)=\sqrt{x^2+5} \) and the selected point \( x_0=1 \), explain
why the truncation error will at most be
$$
\begin{equation}
E_{\text{max}}\approx{0.17h}\,.
\tag{12}
\end{equation}
$$

Compare this with the actual error found using your differentiation routine,
and comment upon the result.

<p>
<b>Part 3.</b>
Add an extra term to the estimate for the error as follows:
$$
\begin{equation}
E_{\text{max}}\approx{0.17h}+\frac{2\epsilon_M}{h}|f(1)|\,.
\tag{13}
\end{equation}
$$

You may assume that \( \epsilon_{M}\sim{10^{-16}} \). Redo the comparison you made
in the previous exercise. What is different now? Explain the meaning of the
second term in the above formula (hint: you may want to read about the
<a href="https://en.wikipedia.org/wiki/Machine_epsilon" target="_self">machine epsilon</a>)

<p>
<b>Part 4.</b>
An estimate for the value of \( h \) at which the total error reaches its
minimum is given by:
$$
\begin{equation}
h_{min}\approx{2}\sqrt{\frac{|\epsilon_{M}f(x_0)|}{|f^{\prime\prime}(x_0)|}}\,.
\tag{14}
\end{equation}
$$

Explain how to obtain this formula from the above estimate of the total error.
For the currently investigated function and \( x_0 \), calculate \( h_{min} \) and
compare it with the plotted results.

<p>
<b>Part 5.</b>
Use the forward difference method to estimate the derivative \( f^{\prime}(x_0) \)
again, but this time ensure that Python uses 32 bits to represent floating
point numbers. Make a double-logarithmic plot of the error versus step size,
in which you compare the results with what you found for the default
number representation (64 bits).

<p>
What do you see? Approximately what value should you employ for \( \epsilon_M \)
now?

<h1 id="linear_regression_normal_eqs" class="anchor">Exercise 6: (Optional) Multiple linear regression</h1>

<p>
Previously we looked at data consisting of tuples of numbers \( x \) and \( y \),
and we attempted to fit a straight line to describe \( y \) as a function of \( x \).
In reality there tend to be multiple factors affecting the outcome of an
experiment, which means that \( y \) should be a function of more than a single
independent variable.

<p>
To generalize, suppose therefore that we have \( k>1 \) independent variables,
so that each observational data point can be viewed as a vector in
\( k+1 \)-dimensional space: \( k \) x-values, and a single \( y \)-value.
We still want to try and capture the relationship between \( y \)- and the
\( x \)-variables in terms of a linear model, i.e., an expression of the form
$$
\begin{equation}
y\approx\beta_0 + \displaystyle\sum_{j=1}^{k}\beta_{j}x_{j}\,,
\tag{15}
\end{equation}
$$

where \( x_1 \), \( x_2 \), \( \ldots \), \( x_k \) are \( k>1 \) variables in
question.

<p>
Let \( x_{i,j} \) denote the collected value of \( x_j \) for observation \( i \).
As before, the ordinary least squares solution consists in minimizing
the sum of squared residuals. Given a set of 'training data', the expression
to be minimized is
$$
\begin{equation}
\tag{16}
\chi^2=\displaystyle\sum_{i}\left(y_i-\beta_0-
\displaystyle\sum_{j=1}^k\beta_{j}x_{i,j}\right)^2\,.
\end{equation}
$$

<p>
By differentiating <a href="#mjx-eqn-16">(16)</a> with respect to each of
the \( \beta \)'s, setting the obtained expressions to zero, and solving the
resulting linear system of equations, we find an estimated set of 'best fit'
coefficients \( \hat{\beta_0} \), \( \hat{\beta_1} \), \( \ldots \), \( \hat{\beta_k} \).
This system of equations can also be derived in an alternative fashion, using
theory from linear algebra. First, note that the expression to be
minimized can be rewritten in matrix-vector notation as
$$
\begin{align}
\chi^2=\langle \vec{\epsilon}, \vec{\epsilon} \rangle=\|\vec{y}-X\vec{\beta}\|^2\,,
\tag{17}
\end{align}
$$

where \( \vec{\epsilon}=(\epsilon_1, \ldots, \epsilon_{n})^{\top}=\vec{y}-X\vec{\beta} \)
is the vector of residuals, \( \vec{y}=(y_1, \ldots, y_{n})^{\top} \),
\( \vec{\beta}=(\beta_0, \ldots, \beta_{k})^{\top} \), and where \( X \) is the
following matrix:

$$
\begin{equation}
X=
\left[ {\begin{array}{ccccc}
 1 & x_{1,1} & x_{1,2} & \ldots & x_{1,k} \\ 
 1 & x_{2,1} & x_{2,2} & \ldots & x_{2,k} \\ 
 \text{ } & \text{ } & \ldots & \text{ } & \text{ } \\ 
 1 & x_{n,1} & x_{n,2} & \ldots & x_{n,k} \\ 
\end{array} } \right]\,.
\tag{18}
\end{equation}
$$

Note that if we want to neglect the constant term (\( \beta_0=0 \)), we can simply
remove the first column of the matrix \( X \).

<p>
Using the language of linear algebra, we can interpret the least squares
problem geometrically.
We start by noting that the vector \( \vec{y} \) is almost certainly not spanned
by the columns of the matrix \( X \); otherwise the residual would be zero, and
we would have a perfect linear model. Let \( \vec{p} \) be the projection of
\( \vec{y} \) onto the column space Col(X), so that the residual vector becomes
$$
\vec{\epsilon}=\vec{y}-\vec{p}=\vec{y}-X\hat{\beta}\,,
$$

for some vector \( \hat{\beta} \). By definition, the projection is the vector
in Col(X) with the smallest Euclidean distance away from \( \vec{y} \), which
means that \( \hat{\beta} \) is the least squares solution! Also, the residual
vector is normal to the column space, meaning that the dot product between
it and any vector from the column space is zero,
$$
\langle X\beta, \vec{\epsilon} \rangle = \langle X\beta, \vec{y}-X\hat{\beta} \rangle
=\beta^{\top}X^{\top}(\vec{y}-X\hat{\beta})=0
$$

for <em>all</em> vectors \( \vec{\beta} \). Since it is true for all vectors, we
immediately deduce that
$$
X^{\top}(\vec{y}-X\hat{\beta})=0\,,
$$

from which it follows that
$$
\begin{equation}
\tag{19}
X^{\top}X\hat{\beta}=X^{\top}\vec{y}\,.
\end{equation}
$$

Equations <a href="#mjx-eqn-19">(19)</a> are known as the <em>normal equations</em>,
and the system can readily solved as long as the matrix \( X^{T}X \) is
invertible, which will be the case if and only if the columns of \( X \) are
linearly independent.

<p>
<b>Part 1.</b>

<ul>
<li> Implement a custom Python routine in which you solve the ordinary least squares problem for multiple independent variables via the normal equations approach.</li>
</ul>

When it comes to actually solving the system <a href="#mjx-eqn-19">(19)</a>,
you may use, e.g.,
the <code>solve</code> method in
<a href="https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.solve.html" target="_self">scipy.linalg package</a>.

<p>
You may take the following code as a starting point:

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span><span style="color: #008000; font-weight: bold">def</span> <span style="color: #0000FF">OLSR_multiple</span>(X_matrix, y, <span style="color: #666666">*</span>, include_constant_term<span style="color: #666666">=</span><span style="color: #008000">True</span>):
    <span style="color: #BA2121; font-style: italic">&quot;&quot;&quot;</span>
<span style="color: #BA2121; font-style: italic">    Custom implementation of Ordinary Least Squares Regression for</span>
<span style="color: #BA2121; font-style: italic">    multiple predictor variables.</span>

<span style="color: #BA2121; font-style: italic">    This routine computes the &#39;best fit&#39; regression coefficients from</span>
<span style="color: #BA2121; font-style: italic">    training data.</span>

<span style="color: #BA2121; font-style: italic">    :param X_matrix: Matrix holding values of predictor variables.</span>
<span style="color: #BA2121; font-style: italic">              Each column corresponds to a single variable, and the rows</span>
<span style="color: #BA2121; font-style: italic">              correspond to different observations.</span>
<span style="color: #BA2121; font-style: italic">    :param y: Input data for the dependent variable (a single array)</span>
<span style="color: #BA2121; font-style: italic">    :param include_constant_term: If False, force linear model to pass through</span>
<span style="color: #BA2121; font-style: italic">                                  the origin (no intercept term).</span>
<span style="color: #BA2121; font-style: italic">    :return: Array consisting of fitted coefficients to the regression line.</span>
<span style="color: #BA2121; font-style: italic">    &quot;&quot;&quot;</span>
</pre></div>
<p>
<b>Part 2.</b>

<ul>
<li> Implement a Python function that takes as input a matrix of \( x \)-values and a given choice of fitted regression coefficients, and returns a prediction for the corresponding \( y \)-values.</li>
</ul>

<b>Part 3.</b>

<ul>
<li> Test your first routine by feeding in an \( y \)-variable which is an exact linear function of the \( x \)-variables, and check that you get back the known answer for the regression coefficients. For example:</li>
</ul>

<p>

<!-- code=python (!bc pycod) typeset with pygments style "default" -->
<div class="highlight" style="background: #f8f8f8"><pre style="line-height: 125%"><span></span>X <span style="color: #666666">=</span> np<span style="color: #666666">.</span>array([[<span style="color: #666666">1</span>, <span style="color: #666666">1</span>], [<span style="color: #666666">1</span>, <span style="color: #666666">2</span>], [<span style="color: #666666">2</span>,<span style="color: #666666">2</span>], [<span style="color: #666666">2</span>,<span style="color: #666666">3</span>]])
y <span style="color: #666666">=</span> np<span style="color: #666666">.</span>dot(X, np<span style="color: #666666">.</span>array([<span style="color: #666666">1</span>,<span style="color: #666666">2</span>]))<span style="color: #666666">+3</span>  <span style="color: #408080; font-style: italic"># y = 3 + x1 + 2*x2</span>
regr_coeffs <span style="color: #666666">=</span> OLSR_multiple(X, y)
<span style="color: #408080; font-style: italic"># more code here..</span>
</pre></div>
<p>
<b>Part 4.</b>

<ul>
<li> Perform a multiple linear regression (in the least squares sense) on the data from the file <em>xyz\_data.dat</em>, assuming that \( z \) is essentially a linear function of \( x \) and \( y \). For data point \( i \), let</li>
</ul>

$$
\begin{equation}
z_i=\beta_{1}\cdot{x_i}+\beta_{2}\cdot{y_i}+\epsilon_i\,.
\tag{20}
\end{equation}
$$

Notice that we do not include a constant term here.

<ul>
<li> Report the 'best-fit' estimate for the regression coefficients \( \hat{\beta}_1 \) and \( \hat{\beta}_2 \), as well as the value of <em>R-squared</em> for the fit.</li>
<li> Again, to test your implementation you might want to compare with results from another Python library.</li>
</ul>

<b>Part 5.</b>

<ul>
<li> Read another set of data from the file <em>xyz\_data2.dat</em>, and apply your linear model to predict the value of \( z \) from knowledge of \( x \) and \( y \).</li>
<li> Compare your prediction with the known true result. Again, compute \( R^2 \).</li>
</ul>

<b>Part 6.</b>

<p>
In reality, \( z \) was computed as \( z=\sqrt{x^2+y^2} \), and the input \( x \)- and
\( y \)-values were randomly drawn from the interval \( [0,20] \).

<ul>
<li> Test your linear model again, but this time draw 50 random \( x \)- and \( y \)-values from the interval \( [-20,0] \), and compute the true value for \( z \) based on that.</li>
<li> How does your prediction match the real model now, and what does it tell you about the ability of \( R^2 \) to judge the appropriateness of model selection; in this case, an assumed linear dependency of \( z \) on \( x \) and \( y \)?</li>
</ul>

<b>Part 7.</b>

<ul>
<li> What happens if you redo Part 6, but switch the signs of the obtained regression coefficients?</li>
<li> Why?</li>
</ul>

Hint: Taylor expand the multivariable function \( g(x,y)=\sqrt{x^2+y^2} \) around
a point \( (x_0,y_0) \), and consider, e.g., what happens when \( x_0=y_0 \)

<h1 id="___sec6" class="anchor">Guidelines for project submission </h1>
The assignment is provided both as a PDF, and as a Jupyter notebook.
However, the work done to answer the exercises only has to be handed in as a
notebook, though you can submit an additional PDF if you want.
You should bear the following points in mind when working on the project:

<ul>
<li> Start your notebook by providing a short introduction in which you outline the nature of the problem(s) to be investigated.</li>
<li> End your notebook with a brief summary of what you feel you learnt from the project (if anything). Also, if you have any general comments or suggestions for what could be improved in future assignments, this is the place to do it.</li>
<li> All code that you make use of should be present in the notebook, and it should ideally execute without any errors (especially run-time errors). If you are not able to fix everything before the deadline, you should give your best understanding of what is not working, and how you might go about fixing it.</li>
<li> If you use an algorithm that is not fully described in the assignment text, you should try to explain it in your own words. This also applies if the method is described elsewhere in the course material.</li>
<li> In some cases it may suffice to explain your work via comments in the code itself, but other times you might want to include a more elaborate explanation in terms of, e.g., mathematics and/or pseudocode.</li>
<li> In general, it is a good habit to comment your code (though it can be overdone).</li>
<li> When working with approximate solutions to equations, it is always useful to check your results against known exact (analytical) solutions, should they be available.</li>
<li> It is also a good test of a model implementation to study what happens at known 'edge cases'.</li>
<li> Any figures you include should be easily understandable. You should label axes appropriately, and depending on the problem, include other legends etc. Also, you should discuss your figures in the main text.</li>
<li> It is always good if you can reflect a little bit around <em>why</em> you see what you see.</li>
</ul>


<p>
<!-- navigation buttons at the bottom of the page -->
<ul class="pager">

  <li class="previous">
    <a href="._project1-readable000.html">&larr; Prev</a>
  </li>
</ul>
<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


</body>
</html>
    

