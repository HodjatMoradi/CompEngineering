{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: MOD510: Mandatory project \\#1 -->\n",
    "# MOD510: Mandatory project \\#1\n",
    "<!-- dom:AUTHOR: Deadline Sun 15 September 2019 -->\n",
    "<!-- Author: -->  \n",
    "**Deadline Sun 15 September 2019**\n",
    "\n",
    "Date: **Aug 28, 2019**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning objectives.**\n",
    "* Combine and manipulate data from different input (Excel) files.\n",
    "\n",
    "* Visualize data using [`matplotlib`](https://matplotlib.org).\n",
    "\n",
    "* Use [Taylor's theorem](https://en.wikipedia.org/wiki/Taylor%27s_theorem) to approximate numerical derivatives.\n",
    "\n",
    "* Fit straight lines to data using the method of least squares.\n",
    "\n",
    "* Study the impact of numerical errors on computed derivatives.\n",
    "\n",
    "* Compare your own implementation of the least squares algorithm with functions in available Python libraries.\n",
    "\n",
    "* (Optional) Implement and use multiple linear regression to fit a linear model to a dataset in which the 'true model' is known to be non-linear.\n",
    "\n",
    "The work done to answer the exercises should be documented clearly in a\n",
    "handed-in Jupyter notebook.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Exercise 1: Visualize oil production data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "When analyzing data it is very advantageous to start by making a plot.\n",
    "The human mind is usually good at detecting patterns, and by looking at\n",
    "the data one can think of simple ideas to test out, before possibly\n",
    "doing a more comprehensive analysis.\n",
    "\n",
    "As part of this project we will look at some of the datasets that\n",
    "are available at the Norwegian Petroleum Directorate (NPD)\n",
    "[website](http://factpages.npd.no/factpages/).\n",
    "These data are updated regularly, and if you are able to make a good model\n",
    "of, e.g., the historical oil production versus time, you could use it to\n",
    "forecast the production in the future as well.\n",
    "\n",
    "\n",
    "**Part 1.**\n",
    "\n",
    "\n",
    "A challenge when doing data analysis is that the data is usually not available\n",
    "in the format you would like it to be in. In Python there are many ways of\n",
    "preprocessing and manipulating data, but the\n",
    "[`Pandas`](https://pandas.pydata.org) library is particularly well-suited\n",
    "for the purpose.\n",
    "Below is a code snippet to help you to get started with reading data from Excel\n",
    "into a `Pandas` [`DataFrame`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def df_field(name, datafile='field_production_gross_monthly.xls', col=0):\n",
    "    folder = pathlib.Path.cwd().parent.joinpath('data')\n",
    "    filename = folder.joinpath(datafile)\n",
    "    df = pd.read_excel(filename)\n",
    "    columns = df.columns\n",
    "    return df[df[df.columns[col]] == name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that all data files needed for this project are located in the `data`\n",
    "folder.\n",
    "\n",
    "* Explain in your own words what the above Python function does.\n",
    "\n",
    "* Open the file `field_production_gross_monthly.xls`, and compare its contents with output from the command `print(df_field('OSEBERG'))`.\n",
    "\n",
    "**Part 2.**\n",
    "\n",
    "Next, we wish to look at the historical oil production versus time for a\n",
    "specific field. Such plots are already available at the\n",
    "[NPD website](http://factpages.npd.no/factpages/), but here you are going to\n",
    "make your own by using the [`matplotlib`](https://matplotlib.org) library.\n",
    "\n",
    "When we develop code, it is good practice to use functions to divide a\n",
    "problem into smaller pieces. We start by extracting oil production data\n",
    "for a specific field with the function already defined in\n",
    "Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prod_data(name):\n",
    "    df = df_field(name)\n",
    "    columns = df.columns\n",
    "    Year    = df[ ... ]\n",
    "    Month   = df[ ... ]\n",
    "    OilProd = df[ ... ]\n",
    "    #Assume 30 days in each month and 365 in year\n",
    "    Year    = Year + Month*30/365\n",
    "    return Year, OilProd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Complete writing the above function.\n",
    "\n",
    "**Part 3.**\n",
    "* We also make a function that plots the oil production versus time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prod_data(name):\n",
    "    Year, OilProd=prod_data(name)\n",
    "    ...  # make plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Finish implementing this Python function as well.\n",
    "\n",
    "Calling `plot_prod_data('OSEBERG')` should generate a figure that looks\n",
    "something like this:\n",
    "\n",
    "<!-- dom:FIGURE: [fig-project1/oseberg.png, width=400 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"fig-project1/oseberg.png\" width=400>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "\n",
    "**Part 4 (Optional).**\n",
    "In the oil production plot for a given field, we additionally want to include\n",
    "information about the number of wells that have been drilled. Specifically,\n",
    "we wish to plot, on a separate $y$-axis, the cumulative number of wells present\n",
    "in the field at any given moment in time.\n",
    "\n",
    "Well data is stored in the file `wellbore_development_all.xls`.\n",
    "In this file, each row entry corresponds to a single well, and the\n",
    "field to which it belongs is stored in the column with index 14.\n",
    "Hence, we can start by writing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prod_and_well_data(name):\n",
    "    fn = 'wellbore_development_all.xls'\n",
    "    df = df_field(name, datafile=fn, col=14)\n",
    "    columns = df.columns\n",
    "    year = df['Completed year']  # alternatively: fetch by index (32)\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the Excel file only tells you the year in which each well was\n",
    "completed; *there could be many wells drilled in a given year, and the list is\n",
    "not sorted*. This means that you need to a little more work to process the\n",
    "data:\n",
    "\n",
    "* Finish implementing the above Python function.\n",
    "\n",
    "* Calling it with a specific field as input should generate a plot that looks something like this:\n",
    "\n",
    "<!-- dom:FIGURE: [fig-project1/oseberg2.png, width=800 frac=1.0] -->\n",
    "<!-- begin figure -->\n",
    "\n",
    "<p></p>\n",
    "<img src=\"fig-project1/oseberg2.png\" width=800>\n",
    "\n",
    "<!-- end figure -->\n",
    "\n",
    "\n",
    "If you want, you can also distinguish between different types of wells, i.e.,\n",
    "between injection/production/observation wells.\n",
    "\n",
    "\n",
    "\n",
    "# Exercise 2: More wells, more oil?\n",
    "\n",
    "\n",
    "A very common statement is \"more wells - more oil\". The fields on the Norwegian\n",
    "Continental Shelf (NCS) are in different stages of production, and they have\n",
    "varying reservoir properties, so a thorough analysis would take time.\n",
    "However, we would at least expect to see some correlation between the\n",
    "*total amount of oil produced* and *the number of wells drilled*, if the\n",
    "hypothesis is correct.\n",
    "\n",
    "\n",
    "\n",
    "**Part 1.**\n",
    "The first task is to fetch the (final) number of wells drilled for each field,\n",
    "as well as the total (cumulative) oil production. The resulting values are\n",
    "to be stored in arrays:\n",
    "\n",
    "* The hard way is to do this yourself by extracting the data you need from a combination of the provided NPD `.xls` files.\n",
    "\n",
    "* Alternatively, you can 'cheat' and use the preprocessed data stored in `fields_oil_wells.xlsx`.\n",
    "\n",
    "**Part 2.**\n",
    "\n",
    "\n",
    "* Make a scatter plot showing the cumulative oil production of all the fields on the $y$-axis. On the $x$-axis, plot the number of wells for the fields (you might want to exclude observation wells).\n",
    "\n",
    "(We will return to this data set later in this project, after introducing linear regression.) \n",
    "\n",
    "\n",
    "**Part 3.**\n",
    "The Draugen field is very homogeneous, and it is therefore regarded as an ideal\n",
    "field on the NCS. It also has an active aquifer underlying the reservoir,\n",
    "meaning that when producing oil by water injection the aquifer provides\n",
    "additional pressure support.\n",
    "\n",
    "* Does the plot you made in Part 2 indicate that Draugen has been a successful field compared to the others? Why/why not?\n",
    "\n",
    "# Exercise 3: Finite differences\n",
    "\n",
    "\n",
    "The most straightforward way to approximate the derivative of a function\n",
    "$f=f(x)$ is to use the function value at $x$ *and* at a small distance $h$\n",
    "from $x$, e.g.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f^{\\prime}(x)\\approx\\frac{f(x+h)-f(x)}{h}\\,.\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approximation is called the *forward difference*.\n",
    "For 'well-behaved' functions, it can be shown using Taylor's formula\n",
    "that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:fder_fd\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:fder_fd} \\tag{2}\n",
    "f^{\\prime}(x)=\\frac{f(x+h)-f(x)}{h}-h\\frac{f^{\\prime\\prime}(\\xi)}{2}\\,,\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for some $\\xi$ between $x$ and $x+h$.\n",
    "\n",
    "\n",
    "\n",
    "**Part 1.**\n",
    "Explain in detail how equation ([2](#eq:fder_fd)) can be derived from Taylor's\n",
    "formula.\n",
    "\n",
    "What is the order of the truncation error for the forward difference numerical\n",
    "differentiation method? What should ideally happen if you lower the step size\n",
    "by a factor of ten?\n",
    "\n",
    "\n",
    "**Part 2.**\n",
    "Alternatively, we could use the *backward difference* approximation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f^{\\prime}(x)\\approx\\frac{f(x)-f(x-h)}{h}\\,.\n",
    "\\label{_auto2} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this method, use Taylor expansions to derive a similar expression\n",
    "as ([2](#eq:fder_fd)).\n",
    "\n",
    "What is the truncation error for this method?\n",
    "\n",
    "\n",
    "\n",
    "**Part 3.**\n",
    "The *central difference* approximation of the derivative is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto3\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f^{\\prime}(x)\\approx\\frac{f(x+h)-f(x-h)}{2h}\\,.\n",
    "\\label{_auto3} \\tag{4}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, use Taylor expansions to derive this expression. What is the truncation\n",
    "error, and how does it compare to the other two methods?\n",
    "(hint: combine the derivations you did for the forward and backward\n",
    "finite difference schemes)\n",
    "\n",
    "\n",
    "\n",
    "**Part 4.**\n",
    "Implement a Python function that can calculate numerical derivatives of\n",
    "functions $f=f(x)$. The routine should be able to handle all of the three\n",
    "aforementioned formulas (forward difference, backward difference, central\n",
    "difference), as well as different values of the step size, $h$.\n",
    "\n",
    "You may take the following code as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x, *, method='forward', h=1.0e-4):\n",
    "    \"\"\"\n",
    "    Function to numerically evaluate the derivative of a function\n",
    "    f=f(x).\n",
    "\n",
    "    :param f: Function to differentiate.\n",
    "    :param x: Point at which to evaluate the derivative.\n",
    "    :param method: Approximation formula to apply. Available options\n",
    "                   are 'forward' (default), 'backward', and 'central'\n",
    "                   finite differences.\n",
    "    :param h: Step size (default: 1.0e-4).\n",
    "    :return: Approximate value for f'(x).\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 5.**\n",
    "\n",
    "Consider the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x)=\\sqrt{x^2+5}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate the numerical error when calculating $f^\\prime(1)$ with\n",
    "a finite difference formula:\n",
    "\n",
    "* First, show that the exact value of the derivative is $f^\\prime(1)=\\frac{1}{\\sqrt{6}}$.\n",
    "\n",
    "* Apply your implemented Python routine to numerically evaluate $f^{\\prime}(1)$. Do this for $h=10^{-16}, h=10^{-15}, \\ldots$, up to $h=10^{-1}$, and for all of the three introduced finite difference formulas.\n",
    "\n",
    "* For each differentiation method, make a figure showing the absolute value of the error versus $h$ on a double-logarithmic plot.\n",
    "\n",
    "# Exercise 4: Fitting a simple linear model to data\n",
    "\n",
    "When dealing with physical data points, they rarely fit a smooth curve, but\n",
    "we would still like to calculate derivatives to report, e.g., how\n",
    "fast a quantity is changing as a function of time. One way of achieving this\n",
    "is to calculate the derivative directly from the data, using a numerical\n",
    "differentiation formula. However, such an approach can be very sensitive to\n",
    "the choice of points at which we evaluate the derivative.\n",
    "\n",
    "Another approach is to fit a straight line to the data, and report the slope\n",
    "of the line. Suppose we are given $N$ data points ($x_i$, $y_i$), and that\n",
    "we wish to approximate $y$ (the dependent variable) as a linear function of\n",
    "$x$ (the independent variable), i.e.:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y\\approx \\alpha+\\beta\\cdot{x}\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common algorithm for doing this is the method of\n",
    "[least squares](https://en.wikipedia.org/wiki/Least_squares).\n",
    "The idea here is to select the slope $\\beta$ and the intercept $\\alpha$ in\n",
    "such a way that the sum of squares of distances from the line to the data\n",
    "points are minimized. Mathematically, we need to make the following\n",
    "expression as small as possible:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:chi_squared\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:chi_squared} \\tag{5}\n",
    "\\chi^2=\\displaystyle\\sum_{i=1}^{N}\\left(y_i-\\alpha-\\beta\\cdot{x_i}\\right)^2\\,.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the minimum, we can differentiate ([5](#eq:chi_squared)) with respect\n",
    "to both $\\alpha$ and $\\beta$, and set the resulting expressions equal to zero.\n",
    "If we do that, it is possible to show that we must have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:OLSR_intercept\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:OLSR_intercept} \\tag{6}\n",
    "\\alpha=\\overline{y}-\\beta\\overline{x}\\,,\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:OLSR_slope\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:OLSR_slope} \\tag{7}\n",
    "\\beta=\\frac{\\displaystyle\\sum_{i=1}^{N}(x_i-\\overline{x})(y_i-\\overline{y})}\n",
    "{\\displaystyle\\sum_{i=1}^{N}(x_i-\\overline{x})^2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in which $\\overline{x}$ and $\\overline{y}$ are arithmetic averages of\n",
    "the $x-$ and $y$-values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto4\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\overline{x} = \\frac{1}{N}\\displaystyle\\sum_{i=1}^{N} x_{i} \n",
    "\\label{_auto4} \\tag{8}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto5\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}  \n",
    "\\overline{y} = \\frac{1}{N}\\displaystyle\\sum_{i=1}^{N} y_{i} \\,.\n",
    "\\label{_auto5} \\tag{9}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm is also referred to as *ordinary least squares\n",
    "regression* (OLSR), or *simple linear regression*. It is a standard\n",
    "technique used in statistical analyses of empirical data.\n",
    "Often, the goal is to be able to predict the outcome $y$ based on a\n",
    "knowledge of $x$.\n",
    "\n",
    "\n",
    "\n",
    "**Part 1.**\n",
    "\n",
    "* Finish the implementation of the following Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLSR(x, y):\n",
    "    \"\"\"\n",
    "    Custom implementation of Ordinary Least Squares Regression.\n",
    "\n",
    "    :param x: Input data points for the independent variable (array)\n",
    "    :param y: Input data points for the dependent variable (array)\n",
    "    :return: Tuple consisting of (intercept, slope) of the regression line.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2.**\n",
    "* Test the correctness of your implementation by applying the function to a fabricated data set of your own choosing in which the relationship between $x$ and $y$ is exactly linear.\n",
    "\n",
    "**Part 3.**\n",
    "\n",
    "The goodness of fit of a statistical model is frequently evaluated by\n",
    "considering the\n",
    "[coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination),\n",
    "or $R^2$ (*R-squared*). The purpose of $R^2$ is to measure the proportion of\n",
    "variation in the dependent variable that can be explained by variation in the\n",
    "independent variable(s). For the cases investigated in this project, the\n",
    "relevant formula is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto6\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "R^2=1-\\frac{\\displaystyle\\sum_{i=1}^{N}(y_i-\\hat{y_i})^2}\n",
    "{\\displaystyle\\sum_{i=1}^{N}(y_i-\\overline{y})^2}\\,,\n",
    "\\label{_auto6} \\tag{10}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in which $\\hat{y_i}$ is the prediction of the model for data point $i$.\n",
    "For least squares regression models it can be shown that $R^2=\\rho^2$,\n",
    "where $\\rho$ is\n",
    "[Pearson's correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient).\n",
    "* Fill in the missing details of the following Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_r_squared(y_obs, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates R^2 for a linear regression model.\n",
    "\n",
    "    :param y_obs: Array of observed values.\n",
    "    :param y_pred: Array of corresponding predicted / calculated values.\n",
    "    :return: Coefficient of deterermination (R^2) for the fit.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4.**\n",
    "We will now investigate the scatter plot you made in Part 2\n",
    "of Exercise 2 in a little more detail:\n",
    "* Extend the plot by including in it a) the best fit straight line to the data in the least squares sense, and b) Pearson's correlation coefficient for the fit.\n",
    "\n",
    "* Do any of the fields seem to stand out clearly from the rest?\n",
    "\n",
    "**Part 5.**\n",
    "\n",
    "* Calculate the slope and intercept of the least squares line again, but this time use available function(s) from established Python libraries. Check that you get the same regression line as when using your own implementation.\n",
    "\n",
    "As an example, you might opt for the\n",
    "[`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "class from the\n",
    "[`scikitlearn`](https://scikit-learn.org/), library or perhaps the polynomial\n",
    "interpolation function `polyfit` available in\n",
    "[`NumPy`](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyfit.html).\n",
    "Whatever you choose, make sure that you read the documentation and explain in\n",
    "your own words what you do as you go along.\n",
    "\n",
    "\n",
    "# Exercise 5: Error analysis for finite difference schemes\n",
    "\n",
    "\n",
    "\n",
    "**Part 1.**\n",
    "Consider again the error plots you made for the three finite difference\n",
    "formulas in Part 5 of Exercise 3.\n",
    "In each case, fit a straight line to the right part of the error curve.\n",
    "How do the resulting slopes compare to what you would expect from a\n",
    "theoretical truncation error analysis?\n",
    "\n",
    "Also fit a straight line to the left part of each error curve.\n",
    "\n",
    "\n",
    "\n",
    "**Part 2.**\n",
    "By taking absolute values, it follows from ([2](#eq:fder_fd)) that\n",
    "the truncation error $E(f;x_0;h)$ of the forward difference approximation\n",
    "to the derivative at $x=x_0$ must be bounded above by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:fder_fd_upper_bound_abs_error\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:fder_fd_upper_bound_abs_error} \\tag{11}\n",
    "E(f;x_0;h)\\leq\\frac{h}{2}\\cdot\\text{max}_{x\\in[x_0,x_0+h]}|f^{\\prime\\prime}(x)|\\,.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the function $f(x)=\\sqrt{x^2+5}$ and the selected point $x_0=1$, explain\n",
    "why the truncation error will at most be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto7\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E_{\\text{max}}\\approx{0.17h}\\,.\n",
    "\\label{_auto7} \\tag{12}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this with the actual error found using your differentiation routine,\n",
    "and comment upon the result.\n",
    "\n",
    "\n",
    "\n",
    "**Part 3.**\n",
    "Add an extra term to the estimate for the error as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto8\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E_{\\text{max}}\\approx{0.17h}+\\frac{2\\epsilon_M}{h}|f(1)|\\,.\n",
    "\\label{_auto8} \\tag{13}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may assume that $\\epsilon_{M}\\sim{10^{-16}}$. Redo the comparison you made\n",
    "in the previous exercise. What is different now? Explain the meaning of the\n",
    "second term in the above formula (hint: you may want to read about the\n",
    "[machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon))\n",
    "\n",
    "\n",
    "\n",
    "**Part 4.**\n",
    "An estimate for the value of $h$ at which the total error reaches its\n",
    "minimum is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto9\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "h_{min}\\approx{2}\\sqrt{\\frac{|\\epsilon_{M}f(x_0)|}{|f^{\\prime\\prime}(x_0)|}}\\,.\n",
    "\\label{_auto9} \\tag{14}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain how to obtain this formula from the above estimate of the total error.\n",
    "For the currently investigated function and $x_0$, calculate $h_{min}$ and\n",
    "compare it with the plotted results.\n",
    "\n",
    "\n",
    "\n",
    "**Part 5.**\n",
    "Use the forward difference method to estimate the derivative $f^{\\prime}(x_0)$\n",
    "again, but this time ensure that Python uses 32 bits to represent floating\n",
    "point numbers. Make a double-logarithmic plot of the error versus step size,\n",
    "in which you compare the results with what you found for the default\n",
    "number representation (64 bits).\n",
    "\n",
    "What do you see? Approximately what value should you employ for $\\epsilon_M$\n",
    "now?\n",
    "\n",
    "\n",
    "\n",
    "# Exercise 6: (Optional) Multiple linear regression\n",
    "<div id=\"linear_regression_normal_eqs\"></div>\n",
    "\n",
    "Previously we looked at data consisting of tuples of numbers $x$ and $y$,\n",
    "and we attempted to fit a straight line to describe $y$ as a function of $x$.\n",
    "In reality there tend to be multiple factors affecting the outcome of an\n",
    "experiment, which means that $y$ should be a function of more than a single\n",
    "independent variable.\n",
    "\n",
    "To generalize, suppose therefore that we have $k>1$ independent variables,\n",
    "so that each observational data point can be viewed as a vector in\n",
    "$k+1$-dimensional space: $k$ x-values, and a single $y$-value.\n",
    "We still want to try and capture the relationship between $y$- and the\n",
    "$x$-variables in terms of a linear model, i.e., an expression of the form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto10\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y\\approx\\beta_0 + \\displaystyle\\sum_{j=1}^{k}\\beta_{j}x_{j}\\,,\n",
    "\\label{_auto10} \\tag{15}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $x_1$, $x_2$, $\\ldots$, $x_k$ are $k>1$ variables in\n",
    "question.\n",
    "\n",
    "Let $x_{i,j}$ denote the collected value of $x_j$ for observation $i$.\n",
    "As before, the ordinary least squares solution consists in minimizing\n",
    "the sum of squared residuals. Given a set of 'training data', the expression\n",
    "to be minimized is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:chi_squared_multilinear\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:chi_squared_multilinear} \\tag{16}\n",
    "\\chi^2=\\displaystyle\\sum_{i}\\left(y_i-\\beta_0-\n",
    "\\displaystyle\\sum_{j=1}^k\\beta_{j}x_{i,j}\\right)^2\\,.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By differentiating ([16](#eq:chi_squared_multilinear)) with respect to each of\n",
    "the $\\beta$'s, setting the obtained expressions to zero, and solving the\n",
    "resulting linear system of equations, we find an estimated set of 'best fit'\n",
    "coefficients $\\hat{\\beta_0}$, $\\hat{\\beta_1}$, $\\ldots$, $\\hat{\\beta_k}$.\n",
    "This system of equations can also be derived in an alternative fashion, using\n",
    "theory from linear algebra. First, note that the expression to be\n",
    "minimized can be rewritten in matrix-vector notation as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto11\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^2=\\langle \\vec{\\epsilon}, \\vec{\\epsilon} \\rangle=\\|\\vec{y}-X\\vec{\\beta}\\|^2\\,,\n",
    "\\label{_auto11} \\tag{17}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\vec{\\epsilon}=(\\epsilon_1, \\ldots, \\epsilon_{n})^{\\top}=\\vec{y}-X\\vec{\\beta}$\n",
    "is the vector of residuals, $\\vec{y}=(y_1, \\ldots, y_{n})^{\\top}$,\n",
    "$\\vec{\\beta}=(\\beta_0, \\ldots, \\beta_{k})^{\\top}$, and where $X$ is the\n",
    "following matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto12\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "X=\n",
    "\\left[ {\\begin{array}{ccccc}\n",
    " 1 & x_{1,1} & x_{1,2} & \\ldots & x_{1,k} \\\\ \n",
    " 1 & x_{2,1} & x_{2,2} & \\ldots & x_{2,k} \\\\ \n",
    " \\text{ } & \\text{ } & \\ldots & \\text{ } & \\text{ } \\\\ \n",
    " 1 & x_{n,1} & x_{n,2} & \\ldots & x_{n,k} \\\\ \n",
    "\\end{array} } \\right]\\,.\n",
    "\\label{_auto12} \\tag{18}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if we want to neglect the constant term ($\\beta_0=0$), we can simply\n",
    "remove the first column of the matrix $X$.\n",
    "\n",
    "Using the language of linear algebra, we can interpret the least squares\n",
    "problem geometrically.\n",
    "We start by noting that the vector $\\vec{y}$ is almost certainly not spanned\n",
    "by the columns of the matrix $X$; otherwise the residual would be zero, and\n",
    "we would have a perfect linear model. Let $\\vec{p}$ be the projection of\n",
    "$\\vec{y}$ onto the column space Col(X), so that the residual vector becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\vec{\\epsilon}=\\vec{y}-\\vec{p}=\\vec{y}-X\\hat{\\beta}\\,,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for some vector $\\hat{\\beta}$. By definition, the projection is the vector\n",
    "in Col(X) with the smallest Euclidean distance away from $\\vec{y}$, which\n",
    "means that $\\hat{\\beta}$ is the least squares solution! Also, the residual\n",
    "vector is normal to the column space, meaning that the dot product between\n",
    "it and any vector from the column space is zero,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\langle X\\beta, \\vec{\\epsilon} \\rangle = \\langle X\\beta, \\vec{y}-X\\hat{\\beta} \\rangle\n",
    "=\\beta^{\\top}X^{\\top}(\\vec{y}-X\\hat{\\beta})=0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for *all* vectors $\\vec{\\beta}$. Since it is true for all vectors, we\n",
    "immediately deduce that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "X^{\\top}(\\vec{y}-X\\hat{\\beta})=0\\,,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from which it follows that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:normal_eqs_least_squares\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\label{eq:normal_eqs_least_squares} \\tag{19}\n",
    "X^{\\top}X\\hat{\\beta}=X^{\\top}\\vec{y}\\,.\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations ([19](#eq:normal_eqs_least_squares)) are known as the *normal equations*,\n",
    "and the system can readily solved as long as the matrix $X^{T}X$ is\n",
    "invertible, which will be the case if and only if the columns of $X$ are\n",
    "linearly independent.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Part 1.**\n",
    "* Implement a custom Python routine in which you solve the ordinary least squares problem for multiple independent variables via the normal equations approach.\n",
    "\n",
    "When it comes to actually solving the system ([19](#eq:normal_eqs_least_squares)),\n",
    "you may use, e.g.,\n",
    "the `solve` method in\n",
    "[scipy.linalg package](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.solve.html).\n",
    "\n",
    "You may take the following code as a starting point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OLSR_multiple(X_matrix, y, *, include_constant_term=True):\n",
    "    \"\"\"\n",
    "    Custom implementation of Ordinary Least Squares Regression for\n",
    "    multiple predictor variables.\n",
    "\n",
    "    This routine computes the 'best fit' regression coefficients from\n",
    "    training data.\n",
    "\n",
    "    :param X_matrix: Matrix holding values of predictor variables.\n",
    "              Each column corresponds to a single variable, and the rows\n",
    "              correspond to different observations.\n",
    "    :param y: Input data for the dependent variable (a single array)\n",
    "    :param include_constant_term: If False, force linear model to pass through\n",
    "                                  the origin (no intercept term).\n",
    "    :return: Array consisting of fitted coefficients to the regression line.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 2.**\n",
    "\n",
    "* Implement a Python function that takes as input a matrix of $x$-values and a given choice of fitted regression coefficients, and returns a prediction for the corresponding $y$-values.\n",
    "\n",
    "**Part 3.**\n",
    "\n",
    "* Test your first routine by feeding in an $y$-variable which is an exact linear function of the $x$-variables, and check that you get back the known answer for the regression coefficients. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 1], [1, 2], [2,2], [2,3]])\n",
    "y = np.dot(X, np.array([1,2]))+3  # y = 3 + x1 + 2*x2\n",
    "regr_coeffs = OLSR_multiple(X, y)\n",
    "# more code here.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 4.**\n",
    "* Perform a multiple linear regression (in the least squares sense) on the data from the file *xyz\\_data.dat*, assuming that $z$ is essentially a linear function of $x$ and $y$. For data point $i$, let"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto13\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "z_i=\\beta_{1}\\cdot{x_i}+\\beta_{2}\\cdot{y_i}+\\epsilon_i\\,.\n",
    "\\label{_auto13} \\tag{20}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we do not include a constant term here.\n",
    "\n",
    "* Report the 'best-fit' estimate for the regression coefficients $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$, as well as the value of *R-squared* for the fit.\n",
    "\n",
    "* Again, to test your implementation you might want to compare with results from another Python library.\n",
    "\n",
    "**Part 5.**\n",
    "* Read another set of data from the file *xyz\\_data2.dat*, and apply your linear model to predict the value of $z$ from knowledge of $x$ and $y$.\n",
    "\n",
    "* Compare your prediction with the known true result. Again, compute $R^2$.\n",
    "\n",
    "**Part 6.**\n",
    "\n",
    "In reality, $z$ was computed as $z=\\sqrt{x^2+y^2}$, and the input $x$- and\n",
    "$y$-values were randomly drawn from the interval $[0,20]$.\n",
    "\n",
    "* Test your linear model again, but this time draw 50 random $x$- and $y$-values from the interval $[-20,0]$, and compute the true value for $z$ based on that.\n",
    "\n",
    "* How does your prediction match the real model now, and what does it tell you about the ability of $R^2$ to judge the appropriateness of model selection; in this case, an assumed linear dependency of $z$ on $x$ and $y$?\n",
    "\n",
    "**Part 7.**\n",
    "* What happens if you redo Part 6, but switch the signs of the obtained regression coefficients?\n",
    "\n",
    "* Why?\n",
    "\n",
    "Hint: Taylor expand the multivariable function $g(x,y)=\\sqrt{x^2+y^2}$ around\n",
    "a point $(x_0,y_0)$, and consider, e.g., what happens when $x_0=y_0$\n",
    "\n",
    "\n",
    "# Guidelines for project submission\n",
    "The assignment is provided both as a PDF, and as a Jupyter notebook.\n",
    "However, the work done to answer the exercises only has to be handed in as a\n",
    "notebook, though you can submit an additional PDF if you want.\n",
    "You should bear the following points in mind when working on the project:\n",
    "* Start your notebook by providing a short introduction in which you outline the nature of the problem(s) to be investigated.\n",
    "\n",
    "* End your notebook with a brief summary of what you feel you learnt from the project (if anything). Also, if you have any general comments or suggestions for what could be improved in future assignments, this is the place to do it.\n",
    "\n",
    "* All code that you make use of should be present in the notebook, and it should ideally execute without any errors (especially run-time errors). If you are not able to fix everything before the deadline, you should give your best understanding of what is not working, and how you might go about fixing it.\n",
    "\n",
    "* If you use an algorithm that is not fully described in the assignment text, you should try to explain it in your own words. This also applies if the method is described elsewhere in the course material.\n",
    "\n",
    "* In some cases it may suffice to explain your work via comments in the code itself, but other times you might want to include a more elaborate explanation in terms of, e.g., mathematics and/or pseudocode.\n",
    "\n",
    "* In general, it is a good habit to comment your code (though it can be overdone).\n",
    "\n",
    "* When working with approximate solutions to equations, it is always useful to check your results against known exact (analytical) solutions, should they be available.\n",
    "\n",
    "* It is also a good test of a model implementation to study what happens at known 'edge cases'.\n",
    "\n",
    "* Any figures you include should be easily understandable. You should label axes appropriately, and depending on the problem, include other legends etc. Also, you should discuss your figures in the main text.\n",
    "\n",
    "* It is always good if you can reflect a little bit around *why* you see what you see."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
