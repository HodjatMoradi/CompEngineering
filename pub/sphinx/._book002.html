
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml" lang="1.0">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Numerical derivatives &#8212; Modeling and Computational Engineering 1.0 documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Numerical integration" href="._book003.html" />
    <link rel="prev" title="Preface" href="._book001.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  
       <style type="text/css">
         div.admonition {
           background-color: whiteSmoke;
           border: 1px solid #bababa;
         }
       </style>
      </head>
    <body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="numerical-derivatives">
<span id="ch-numder"></span><h1>Numerical derivatives<a class="headerlink" href="#numerical-derivatives" title="Permalink to this headline">¶</a></h1>
<p>The mathematics introduced in this chapter is absolutely essential in order to understant the development of numerical algorithms. We strongly advice you to study it carefully, run the scripts and investigate the results, reproduce the analytical derivations and compare with the numerical solutions.</p>
<div class="section" id="finite-differences">
<h2>Finite Differences<a class="headerlink" href="#finite-differences" title="Permalink to this headline">¶</a></h2>
<p>The solution to a mathemathical model is usually a function. The function could decsribe the temperature evolution of the earth, it could be growth of cancer cells, the water pressure in an oil reservoir, the list is endless. Our models are usually solved by the help of a computer program. The computer does not have any concept of continuous functions, a function is always evaluated at some point in space and/or time. Assume for simplicity that the solution to our problem is <span class="math notranslate nohighlight">\(f(x)=\sin x\)</span>, and we would like to visualize the solution. How many points do we need in our plot to approximate the true function?
In figure <a class="reference internal" href="#fig-taylor-sinx"><span class="std std-ref">A plot of  \( sin x \)  for different spacing of the  \( x \) -values</span></a>, there is a plot of <span class="math notranslate nohighlight">\(\sin x\)</span> on the interval <span class="math notranslate nohighlight">\([-\pi,\pi]\)</span>.</p>
<div class="figure" id="id1">
<span id="fig-taylor-sinx"></span><a class="reference internal image-reference" href="_images/func_plot.png"><img alt="_images/func_plot.png" src="_images/func_plot.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-text"><em>A plot of  \( sin x \)  for different spacing of the  \( x \) -values</em></span></p>
</div>
<p>From the figure we see that in some areas oly a couple of points are needed in order to
represent the function well, and in some areas more points are needed. To state it more clearly; between <span class="math notranslate nohighlight">\([-1,1]\)</span> a linear function (few points) approximate <span class="math notranslate nohighlight">\(\sin x\)</span> well,
whereas in the area where the derivative of the function changes e.g. in <span class="math notranslate nohighlight">\([-2,-1]\)</span>, we need the points to be more closely spaced to capture the behavior of the true function.</p>
<p>Why do we care about the number of points? In many cases the function we would like to evaluate can take a very long time to evaluate. Sometimes simulation time is not an issue, then we can use a large number of function
evaluations. However, in many applications simulation time <em>is an issue</em>, and it would be good to know where where the points needs to be closely spaced, and where we can
manage with only a few points.</p>
<p>What is a <em>good representation</em> representation of the true function? We cannot rely on visual inspection. In the next section we will show how Taylor polynomial representation of a function is a natural starting point to answer this question.</p>
</div>
<div class="section" id="taylor-polynomial-approximation">
<h2>Taylor Polynomial Approximation<a class="headerlink" href="#taylor-polynomial-approximation" title="Permalink to this headline">¶</a></h2>
<p>There are many ways of representing a function, but perhaps one of the most widely used is Taylor polynomials.
Taylor series are the basis for solving ordinary and differential equations, simply because it makes it possible to evaluate any function with a set
of limited operations: <em>addition, subtraction, and multiplication</em>. The Taylor polynomial, <span class="math notranslate nohighlight">\(P_n(x)\)</span> of degree <span class="math notranslate nohighlight">\(n\)</span> of a function <span class="math notranslate nohighlight">\(f(x)\)</span> at the point <span class="math notranslate nohighlight">\(c\)</span> is defined as:</p>
<div class="admonition-taylor-polynomial admonition">
<p class="first admonition-title">Taylor polynomial</p>
<div class="last math notranslate nohighlight">
\[P_n(x) = f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots+\frac{f^{(n)}(c)}{n!}(x-c)^n\nonumber\]</div>
</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-taylori">
\[\tag{1}
=\sum_{k=0}^n\frac{f^{(n)}}{k!}(x-c)^k.\]</div>
<p>If the series is around the point <span class="math notranslate nohighlight">\(c=0\)</span>, the Taylor polynomial <span class="math notranslate nohighlight">\(P_n(x)\)</span> is often called a Maclaurin polynomial, more examples can be found
<a class="reference external" href="https://en.wikipedia.org/wiki/Taylor_series">here</a>. If the series converge (i.e. that the higher order terms approach zero), then we can represent the
function <span class="math notranslate nohighlight">\(f(x)\)</span> with its corresponding Taylor series around the point <span class="math notranslate nohighlight">\(x=c\)</span>:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-taylor">
\[\tag{2}
f(x) = f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots
    =\sum_{k=0}^\infty\frac{f^{(n)}}{k!}(x-c)^k.\]</div>
<p>The Maclaurin series of <span class="math notranslate nohighlight">\(\sin x\)</span> is:</p>
<div class="math notranslate nohighlight" id="eq-sin">
\[\tag{3}
\sin x = x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\cdots=\sum_{k=0}^{\infty}\frac{(-1)^n}{(2n+1)!}x^{2n+1}.
    \]</div>
<p>In figure <a class="reference internal" href="#fig-mac-sin"><span class="std std-ref">Up to ninth order in the Maclaurin series of </span></a>, we show the first nine terms in the Maclaurin series for <span class="math notranslate nohighlight">\(\sin x\)</span> (all even terms are zero).</p>
<div class="figure" id="id2">
<span id="fig-mac-sin"></span><a class="reference internal image-reference" href="_images/mac_sin.png"><img alt="_images/mac_sin.png" src="_images/mac_sin.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-text">Up to ninth order in the Maclaurin series of <span class="math notranslate nohighlight">\(\sin x\)</span></span></p>
</div>
<p>Note that we get a decent representation of <span class="math notranslate nohighlight">\(\sin x\)</span> on the domain, by <em>only knowing the function and its derivative in a single point</em>.
The error term in Taylors formula, when we represent a function with a finite number of polynomial elements is given by:</p>
<div class="math notranslate nohighlight">
\[R_n(x)=f(x)-P_n(x)=\frac{f^{(n+1)}(\eta)}{(n+1)!}(x-c)^{n+1}\nonumber\]</div>
<div class="math notranslate nohighlight" id="eq-error">
\[\tag{4}
=\frac{1}{n!}\int_c^x(x-\tau)^{n}f^{(n+1)}(\tau)d\tau,\]</div>
<p>for some <span class="math notranslate nohighlight">\(\eta\)</span> in the domain <span class="math notranslate nohighlight">\([x,c]\)</span>.
If we want to calculate
<span class="math notranslate nohighlight">\(\sin x\)</span> to a precision lower than a specified value we can do it as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="c1"># Sinus implementation using the Maclaurin Serie</span>
<span class="c1"># By setting a value for eps this value will be used</span>
<span class="c1"># if not provided</span>
<span class="k">def</span> <span class="nf">my_sin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">eps</span><span class="o">=</span><span class="mf">1e-16</span><span class="p">):</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">power</span> <span class="o">=</span> <span class="n">x</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="o">*</span><span class="n">x</span>
    <span class="n">sign</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">i</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">while</span><span class="p">(</span><span class="n">power</span><span class="o">&gt;=</span><span class="n">eps</span><span class="p">):</span>
        <span class="n">sign</span> <span class="o">=</span> <span class="o">-</span> <span class="n">sign</span>
        <span class="n">power</span> <span class="o">*=</span> <span class="n">x2</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">i</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">f</span> <span class="o">+=</span> <span class="n">sign</span><span class="o">*</span><span class="n">power</span>
        <span class="n">i</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">print</span><span class="p">(</span><span class="s1">&#39;No function evaluations: &#39;</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">f</span>

<span class="n">x</span><span class="o">=</span><span class="mf">0.8</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-9</span>
<span class="k">print</span><span class="p">(</span><span class="n">my_sin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">eps</span><span class="p">),</span> <span class="s1">&#39;error = &#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">-</span><span class="n">my_sin</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">eps</span><span class="p">))</span>
</pre></div>
</div>
<p>This implementation needs some explanation:</p>
<ul class="simple">
<li>The error term is given in equation <a class="reference internal" href="#eq-error"><span class="std std-ref">(4)</span></a>, in this case it reduces to <span class="math notranslate nohighlight">\(R_{2n}=(-1)^{n+1}x^{2n+1}\cos\eta/(2n+1)!\)</span> for terms up to <span class="math notranslate nohighlight">\(k=n-1\)</span> in equation <a class="reference internal" href="#eq-sin"><span class="std std-ref">(3)</span></a>. Since we do not know
where to evaluate <span class="math notranslate nohighlight">\(\eta\)</span> we just replace <span class="math notranslate nohighlight">\(\cos\eta\)</span> with one (since <span class="math notranslate nohighlight">\(\cos\eta\leq1\)</span>). We then add the higher order terms and check if the error term is low enough, since we add the error term
to the function evaluation our estimate will always be better than the specified accuracy.</li>
<li>We evaluate the polynomials in the Taylor series by using the previous values too avoid too many multiplications within the loop, we do this by using the following identity:</li>
</ul>
<div class="math notranslate nohighlight">
\[\sin x=\sum_{k=0}^{\infty} (-1)^nt_n, \text{ where: } t_n\equiv\frac{x^{2n+1}}{(2n+1)!}, \text{ hence :}\nonumber\]</div>
<div class="math notranslate nohighlight">
\[t_{n+1}=\frac{x^{2(n+1)+1}}{(2(n+1)+1)!}=\frac{x^{2n+1}x^2}{(2n+1)! (2n+2)(2n+3)}\nonumber\]</div>
<div class="math notranslate nohighlight" id="eq-auto1">
\[\tag{5}
=t_n\frac{x^2}{(2n+2)(2n+3)}\]</div>
<div class="section" id="evaluation-of-polynomials">
<h3>Evaluation of polynomials<a class="headerlink" href="#evaluation-of-polynomials" title="Permalink to this headline">¶</a></h3>
<p>How to evaluate a polynomial of the type: <span class="math notranslate nohighlight">\(p_n(x)=a_0+a_1x+a_2x^2+\cdots+a_nx^n\)</span>? We already saw a hint in the previous section that it can be done in different ways. One way is simply to
do:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pol</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">pol</span> <span class="o">=</span> <span class="n">pol</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="n">i</span>
</pre></div>
</div>
<p>Note that there are <span class="math notranslate nohighlight">\(n\)</span> additions, whereas there are <span class="math notranslate nohighlight">\(1 + 2 +3+\cdots+n=n(n+1)/2\)</span> multiplications for all the iterations. Instead of evaluating the powers all over in
each loop, we can use the previous calculation to save the number of multiplications:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pol</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">x</span>
<span class="n">power</span> <span class="o">=</span> <span class="n">x</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="n">n</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">power</span>  <span class="o">=</span> <span class="n">power</span><span class="o">*</span><span class="n">x</span>
    <span class="n">pol</span>    <span class="o">=</span> <span class="n">pol</span> <span class="o">+</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">*</span><span class="n">power</span>
</pre></div>
</div>
<p>In this case there are still <span class="math notranslate nohighlight">\(n\)</span> additions, but now there are <span class="math notranslate nohighlight">\(2n-1\)</span> multiplications. For <span class="math notranslate nohighlight">\(n=15\)</span>, this amounts to 120 for the first, and 29 for the second method.
Polynomials can also be evaluated using <em>nested multiplication</em>:</p>
<div class="math notranslate nohighlight">
\[p_1  = a_0+a_1x\nonumber\]</div>
<div class="math notranslate nohighlight">
\[p_2  = a_0+a_1x+a_2x^2=a_0+x(a_1+a_2x)\nonumber\]</div>
<div class="math notranslate nohighlight">
\[p_3  = a_0+a_1x+a_2x^2+a_3x^3=a_0+x(a_1+x(a_2+a_3x))\nonumber\]</div>
<div class="math notranslate nohighlight" id="eq-auto2">
\[\tag{6}
\vdots\]</div>
<p>and so on. This can be implemented as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pol</span> <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">pol</span>  <span class="o">=</span> <span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">pol</span><span class="o">*</span><span class="n">x</span>
</pre></div>
</div>
<p>In this case we only have <span class="math notranslate nohighlight">\(n\)</span> multiplications. So if you know beforehand exactly how many terms is needed to calculate the series, this method would be the preferred method.</p>
</div>
</div>
<div class="section" id="calculating-derivatives-of-functions">
<span id="ch-taylor-der"></span><h2>Calculating Derivatives of Functions<a class="headerlink" href="#calculating-derivatives-of-functions" title="Permalink to this headline">¶</a></h2>
<p>The derivative of a function can be calculated using the definition from calculus:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-der1">
\[\tag{7}
f^\prime(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\simeq \frac{f(x+h)-f(x)}{h}.\]</div>
<p>In the computer we cannot take the limit, <span class="math notranslate nohighlight">\(h\to 0\)</span>, a natural question is then: What value to use for <span class="math notranslate nohighlight">\(h\)</span>?
In figure <a class="reference internal" href="#fig-taylor-df"><span class="std std-ref">Error in the numerical derivative of  \( sin x \)  at  \( x=0.2 \)  for different step size</span></a>, we have evaluated the numerical derivative of <span class="math notranslate nohighlight">\(\sin x\)</span>, using the formula in equation <a class="reference internal" href="#eq-eq-taylor-der1"><span class="std std-ref">(7)</span></a> for different step sizes <span class="math notranslate nohighlight">\(h\)</span>.</p>
<div class="figure" id="id3">
<span id="fig-taylor-df"></span><a class="reference internal image-reference" href="_images/df.png"><img alt="_images/df.png" src="_images/df.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-text"><em>Error in the numerical derivative of  \( sin x \)  at  \( x=0.2 \)  for different step size</em></span></p>
</div>
<p>We clearly see that the error depends on the step size, but there is a minimum; choosing a step size too large give a poor estimate and choosing a too low step size give an
even worse estimate. The explanation for this behavior is two competing effects: <em>mathematical approximation</em> and <em>round off errors</em>. Let us consider approximation or truncation error
first. By using the Taylor expansion in equation <a class="reference internal" href="#eq-eq-taylor-taylor"><span class="std std-ref">(2)</span></a> and expand about <span class="math notranslate nohighlight">\(x\)</span> and the error formula <a class="reference internal" href="#eq-error"><span class="std std-ref">(4)</span></a>, we get:</p>
<div class="math notranslate nohighlight">
\[f(x+h)=f(x)+f^\prime(x)h + \frac{h^2}{2}f^{\prime\prime}(\eta)\text{ , hence:}\nonumber\]</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr">
\[\tag{8}
f^\prime(x)=\frac{f(x+h)-f(x)}{h}-\frac{h}{2}f^{\prime\prime}(\eta),\]</div>
<p>for some <span class="math notranslate nohighlight">\(\eta\)</span> in <span class="math notranslate nohighlight">\([x,x+h]\)</span>. Thus the error to our approximation is <span class="math notranslate nohighlight">\(hf^{\prime\prime}(\eta)/2\)</span>, if we reduce the step size by a factor of 10 the error is reduced by a factor of 10.
Inspecting the graph, we clearly see that this is correct as the step size decreases from <span class="math notranslate nohighlight">\(10^{-1}\)</span> to <span class="math notranslate nohighlight">\(10^{-8}\)</span>. When the step size decreases more, there is an increase in the error. This
is due to round off errors, and can be understood by looking into how numbers are stored in a computer.</p>
<div class="section" id="round-off-errors">
<h3>Round off Errors<a class="headerlink" href="#round-off-errors" title="Permalink to this headline">¶</a></h3>
<p>In a computer a floating point number,$x$, is represented as:</p>
<div class="math notranslate nohighlight" id="eq-auto3">
\[\tag{9}
x=\pm q2^m.\]</div>
<p>Most computers are 64-bits, then one bit is reserved for the sign, 52 for the fraction (<span class="math notranslate nohighlight">\(q\)</span>) and 11 for
the exponent (<span class="math notranslate nohighlight">\(m\)</span>)  (for a graphic illustration see <a class="reference external" href="https://en.wikipedia.org/wiki/Double-precision_floating-point_format">Wikipedia</a>).
what is the largest <em>floating point</em> number the computer can represent?
Since <span class="math notranslate nohighlight">\(m\)</span> contains 11 bits, <span class="math notranslate nohighlight">\(m\)</span> can have the maximal value <span class="math notranslate nohighlight">\(m=2^{11}=1024\)</span>, and then the larges value is close to <span class="math notranslate nohighlight">\(2^{1024}\simeq 10^{308}\)</span>.
If you enter <code class="docutils literal notranslate"><span class="pre">print(10.1*10**(308))</span></code> in Python the answer will be <code class="docutils literal notranslate"><span class="pre">Inf</span></code>. If you enter <code class="docutils literal notranslate"><span class="pre">print(10*10**(308))</span></code>, Python will give an answer. This is because
the number <span class="math notranslate nohighlight">\(10.1\cdot10^{308}\)</span> is floating point number, whereas <span class="math notranslate nohighlight">\(10^{309}\)</span> is an <em>integer</em>, and Python does something clever when it comes to representing integers.
Python has a third numeric type called long int, which can use the available memory to represent an integer.</p>
<p><span class="math notranslate nohighlight">\(10^{308}\)</span> is the largest number, but what is the highest precision we can use, or how many decimal places can we use for a floating point number?
Since there are 52 bits for the fraction, there are <span class="math notranslate nohighlight">\(1/2^{52}\simeq10^{-16}\)</span> decimal places. As an example
the value of <span class="math notranslate nohighlight">\(\pi\)</span> is <span class="math notranslate nohighlight">\(3.14159265358979323846264338\ldots\)</span>, but in Python it can only be represented by 16 digits: <span class="math notranslate nohighlight">\(3.141592653589793\)</span>. In principle
it does not sound so bad to have an answer accurate to 16 digits, and it is much better than most experimental results.
So what is the problem? One problem that you should be aware of is that round off errors can be a serious problem when we subtract two numbers that
are very close to one another. If we implement the following program in Python:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">h</span><span class="o">=</span><span class="mf">1e-16</span>
<span class="n">x</span> <span class="o">=</span> <span class="mf">2.1</span> <span class="o">+</span> <span class="n">h</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">2.1</span> <span class="o">-</span> <span class="n">h</span>
<span class="k">print</span><span class="p">((</span><span class="n">x</span><span class="o">-</span><span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">h</span><span class="p">)</span>
</pre></div>
</div>
<p>we expect to get the answer 2, but instead we get zero. By changing <span class="math notranslate nohighlight">\(h\)</span> to a higher value, the answer will get closer to 2.</p>
<p>Armed with this knowledge of round off errors, we can continue to analyze
the result in figure <a class="reference internal" href="#fig-taylor-df"><span class="std std-ref">Error in the numerical derivative of  \( sin x \)  at  \( x=0.2 \)  for different step size</span></a>.
The round off error when we represent a floating point number <span class="math notranslate nohighlight">\(x\)</span> in the
machine will be of the order <span class="math notranslate nohighlight">\(x/10^{16}\)</span> (<em>not</em> <span class="math notranslate nohighlight">\(10^{-16}\)</span>). In general, when we evaluate a function the error will be of the order
<span class="math notranslate nohighlight">\(\epsilon|f(x)|\)</span>, where <span class="math notranslate nohighlight">\(\epsilon\sim10^{-16}\)</span>. Thus equation <a class="reference internal" href="#eq-eq-taylor-derr"><span class="std std-ref">(8)</span></a> is modified in the following way when we take into account the round off errors:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr2">
\[\tag{10}
f^\prime(x)=\frac{f(x+h)-f(x)}{h}\pm\frac{2\epsilon|f(x)|}{h}-\frac{h}{2}f^{\prime\prime}(\eta),\]</div>
<p>we do not know the sign of the round off error, so the total error <span class="math notranslate nohighlight">\(R_2\)</span> is:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr3">
\[\tag{11}
R_2=\frac{2\epsilon|f(x)|}{h}+\frac{h}{2}|f^{\prime\prime}(\eta)|.\]</div>
<p>We have put absolute values around the function and its derivative to get the maximal error, it might be the case that the round off error cancel part of the
truncation error. However, the round off error is random in nature and will change from machine to machine, and each time we run the program.
Note that the round off error increases when <span class="math notranslate nohighlight">\(h\)</span> decreases, and the approximation error decreases when <span class="math notranslate nohighlight">\(h\)</span> decreases. This is exactly what we see in the figure above. We can find the
best step size, by differentiating <span class="math notranslate nohighlight">\(R_2\)</span> and put it equal to zero:</p>
<div class="math notranslate nohighlight">
\[\frac{dR_2}{dh}=-\frac{2\epsilon|f(x)|}{h^2}+\frac{1}{2}f^{\prime\prime}(\eta)=0\nonumber\]</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr4">
\[\tag{12}
h=2\sqrt{\epsilon\left|\frac{f(x)}{f^{\prime\prime}(\eta)}\right|}\simeq 2\cdot10^{-8},\]</div>
<p>In the last equation we have assumed that <span class="math notranslate nohighlight">\(f(x)\)</span> and its derivative is  \( &nbsp;1 \) . This step size corresponds to an error of order <span class="math notranslate nohighlight">\(R_2\sim10^{-8}\)</span>.
Inspecting
the result in figure <a class="reference internal" href="#fig-taylor-df"><span class="std std-ref">Error in the numerical derivative of  \( sin x \)  at  \( x=0.2 \)  for different step size</span></a>.
we see that the minimum is located at <span class="math notranslate nohighlight">\(h\sim10^{-8}\)</span>.</p>
</div>
</div>
<div class="section" id="higher-order-derivatives">
<h2>Higher Order Derivatives<a class="headerlink" href="#higher-order-derivatives" title="Permalink to this headline">¶</a></h2>
<p>There are more ways to calculate the derivative of a function, than the formula given in equation <a class="reference internal" href="#eq-eq-taylor-derr"><span class="std std-ref">(8)</span></a>. Different formulas can be
derived by using Taylors formula in <a class="reference internal" href="#eq-eq-taylor-taylor"><span class="std std-ref">(2)</span></a>, usually one expands about <span class="math notranslate nohighlight">\(x\pm h\)</span>:</p>
<div class="math notranslate nohighlight">
\[f(x+h)=f(x)+f^\prime(x)h + \frac{h^2}{2}f^{\prime\prime}(x)+ \frac{h^3}{3!}f^{(3)}(x)+ \frac{h^4}{4!}f^{(4)}(x)+\cdots\nonumber\]</div>
<div class="math notranslate nohighlight" id="eq-auto4">
\[\tag{13}
f(x-h)=f(x)-f^\prime(x)h + \frac{h^2}{2}f^{\prime\prime}(x)- \frac{h^3}{3!}f^{(3)}(x)+ \frac{h^4}{4!}f^{(3)}(x)-\cdots.\]</div>
<p>If we add these two equations, we get an expression for the second derivative, because the first derivative cancels out. But we also observe that if we subtract these two equations we get
an expression for the first derivative that is accurate to a higher order than the formula in equation <a class="reference internal" href="#eq-eq-taylor-der1"><span class="std std-ref">(7)</span></a>, hence:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-der2">
\[\tag{14}
f^\prime(x)=\frac{f(x+h)-f(x-h)}{2h} -\frac{h^2}{6}f^{(3)}(\eta),\]</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-2der">
\[\tag{15}
f^{\prime\prime}(x) = \frac{f(x+h)+f(x-h)-2f(x)}{h^2}+ \frac{h^2}{12}f^{(4)}(\eta)\,\]</div>
<p>for some <span class="math notranslate nohighlight">\(\eta\)</span> in <span class="math notranslate nohighlight">\([x,x+h]\)</span>. In figure <a class="reference internal" href="#fig-taylor-df2"><span class="std std-ref">Error in the numerical derivative and second derivative of  \( sin x \)  at  \( x=0.2 \)  for different step size</span></a>, we have plotted equation <a class="reference internal" href="#eq-eq-taylor-derr"><span class="std std-ref">(8)</span></a>, <a class="reference internal" href="#eq-eq-taylor-der2"><span class="std std-ref">(14)</span></a>, and <a class="reference internal" href="#eq-eq-taylor-2der"><span class="std std-ref">(15)</span></a> for
different step sizes. The derivative in equation <a class="reference internal" href="#eq-eq-taylor-der2"><span class="std std-ref">(14)</span></a>, gives a higher accuracy than equation <a class="reference internal" href="#eq-eq-taylor-derr"><span class="std std-ref">(8)</span></a> for a larger step size,
as can bee seen in figure <a class="reference internal" href="#fig-taylor-df2"><span class="std std-ref">Error in the numerical derivative and second derivative of  \( sin x \)  at  \( x=0.2 \)  for different step size</span></a>.</p>
<div class="figure" id="id4">
<span id="fig-taylor-df2"></span><a class="reference internal image-reference" href="_images/df2.png"><img alt="_images/df2.png" src="_images/df2.png" style="width: 600px;" /></a>
<p class="caption"><span class="caption-text"><em>Error in the numerical derivative and second derivative of  \( sin x \)  at  \( x=0.2 \)  for different step size</em></span></p>
</div>
<p>We can perform a similar error analysis as we did before, and then we find for equation <a class="reference internal" href="#eq-eq-taylor-der2"><span class="std std-ref">(14)</span></a> and <a class="reference internal" href="#eq-eq-taylor-2der"><span class="std std-ref">(15)</span></a> that the total
numerical error is:</p>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr3b">
\[\tag{16}
R_3=\frac{\epsilon|f(x)|}{h}+\frac{h^2}{6}f^{(3)}(\eta),\]</div>
<div class="math notranslate nohighlight" id="eq-eq-taylor-derr4b">
\[\tag{17}
R_4=\frac{4\epsilon|f(x)|}{h^2}+\frac{h^2}{12}f^{(4)}(\eta),\]</div>
<p>respectively. Differentiating these two equations with respect to <span class="math notranslate nohighlight">\(h\)</span>, and set the equations equal to zero, we find an optimal step size of
<span class="math notranslate nohighlight">\(h\sim10^{-5}\)</span> for equation <a class="reference internal" href="#eq-eq-taylor-derr3b"><span class="std std-ref">(16)</span></a>, which gives an error of <span class="math notranslate nohighlight">\(R_2\sim 10^{-16}/10^{-5}+(10^{-5})^2/6\simeq10^{-10}\)</span>, and <span class="math notranslate nohighlight">\(h\sim10^{-4}\)</span> for equation
<a class="reference internal" href="#eq-eq-taylor-derr4b"><span class="std std-ref">(17)</span></a>, which gives an error of <span class="math notranslate nohighlight">\(R_4\sim 4\cdot10^{-16}/(10^{-4})^2+(10^{-4})^2/12\simeq10^{-8}\)</span>. Note that we get the surprising result for the first order
derivative in equation <a class="reference internal" href="#eq-eq-taylor-der2"><span class="std std-ref">(14)</span></a>, that a higher step size gives a more accurate result.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Table Of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Numerical derivatives</a><ul>
<li><a class="reference internal" href="#finite-differences">Finite Differences</a></li>
<li><a class="reference internal" href="#taylor-polynomial-approximation">Taylor Polynomial Approximation</a><ul>
<li><a class="reference internal" href="#evaluation-of-polynomials">Evaluation of polynomials</a></li>
</ul>
</li>
<li><a class="reference internal" href="#calculating-derivatives-of-functions">Calculating Derivatives of Functions</a><ul>
<li><a class="reference internal" href="#round-off-errors">Round off Errors</a></li>
</ul>
</li>
<li><a class="reference internal" href="#higher-order-derivatives">Higher Order Derivatives</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="._book001.html" title="previous chapter">Preface</a></li>
      <li>Next: <a href="._book003.html" title="next chapter">Numerical integration</a></li>
  </ul></li>
</ul>
</div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/._book002.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Aksel Hiorth, the National IOR Centre & Institute for Energy Resources,.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.7.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/._book002.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>