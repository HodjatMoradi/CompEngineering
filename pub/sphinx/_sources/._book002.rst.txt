

.. |nbsp| unicode:: 0xA0
   :trim:

.. !split

.. _ch:taylor:

Finite differences
%%%%%%%%%%%%%%%%%%

The mathematics introduced in this chapter is absolutely essential in order to understand the development of numerical algorithms. We strongly advice you to study it carefully, implement python scripts and investigate the results, reproduce the analytical derivations and compare with the numerical solutions.

Numerical derivatives
=====================
The solution to a mathematical model is usually a function. The function could describe the temperature evolution of the earth, it could be growth of cancer cells, the water pressure in an oil reservoir, the list is endless.If we can solve the model analytically, the answer is given in terms of a continuous function. Most of the models cannot be solved analytically, then we have to rely on computers to help us. The computer does not have any concept of continuous functions, a function is always evaluated at some point in space and/or time. Assume for simplicity that the solution to our problem is :math:`f(x)=\sin x`, and we would like to visualize the solution. How many points do we need in our plot to approximate the true function? 
In figure :ref:`fig:taylor:sinx`, there is a plot of :math:`\sin x` on the interval :math:`[-\pi,\pi]`.

.. _fig:taylor:sinx:

.. figure:: func_plot.png
   :width: 600

   *A plot of :math:`\sin x` for different spacing of the :math:`x`-values*

From the figure we see that in some areas only a couple of points are needed in order to
represent the function well, and in some areas more points are needed. To state it more clearly; between :math:`[-1,1]` a linear function (few points) approximate :math:`\sin x` well, 
whereas in the area where the derivative of the function changes e.g. in :math:`[-2,-1]`, we need the points to be more closely spaced to capture the behavior of the true function.

Why do we care about the number of points? In many cases the function we would like to evaluate can take a very long time to evaluate. Sometimes simulation time is not an issue, then we can use a large number of function
evaluations. However, in many applications simulation time *is an issue*, and it would be good to know where the points needs to be closely spaced, and where we can 
manage with only a few points.

What is a *good representation* representation of the true function? We cannot rely on visual inspection. In the next section we will show how Taylor polynomial representation of a function is a natural starting point to answer this question.

Taylor Polynomial Approximation
===============================
There are many ways of representing a function, but perhaps one of the most widely used is Taylor polynomials. 
Taylor series are the basis for solving ordinary and differential equations, simply because it makes it possible to evaluate any function with a set
of limited operations: *addition, subtraction, and multiplication*. The Taylor polynomial, :math:`P_n(x)` of degree :math:`n` of a function :math:`f(x)` at the point :math:`c` is defined as:

.. admonition:: Taylor polynomial

   
   .. math::
           
            P_n(x) = f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots+\frac{f^{(n)}(c)}{n!}(x-c)^n\nonumber
           
   
   .. _Eq:eq:taylor:taylori:

.. math::

    \tag{1}
    =\sum_{k=0}^n\frac{f^{(n)}}{k!}(x-c)^k.\



If the series is around the point :math:`c=0`, the Taylor polynomial :math:`P_n(x)` is often called a Maclaurin polynomial, more examples can be found 
`here <https://en.wikipedia.org/wiki/Taylor_series>`__. If the series converge (i.e. that the higher order terms approach zero), then we can represent the
function :math:`f(x)` with its corresponding Taylor series around the point :math:`x=c`:

.. _Eq:eq:taylor:taylor:

.. math::

    \tag{2}
    f(x) = f(c)+f^\prime(c)(x-c)+\frac{f^{\prime\prime}(c)}{2!}(x-c)^2+\cdots
        =\sum_{k=0}^\infty\frac{f^{(n)}}{k!}(x-c)^k.\
        

The Maclaurin series of :math:`\sin x` is:

.. _Eq:sin:

.. math::

    \tag{3}
    \sin x = x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}+\cdots=\sum_{k=0}^{\infty}\frac{(-1)^n}{(2n+1)!}x^{2n+1}.
        \
        

In figure :ref:`fig:mac_sin`, we show the first nine terms in the Maclaurin series for :math:`\sin x` (all even terms are zero). 

.. _fig:mac_sin:

.. figure:: mac_sin.png
   :width: 600

   Up to ninth order in the Maclaurin series of :math:`\sin x`

Note that we get a decent representation of :math:`\sin x` on the domain, by *only knowing the function and its derivative in a single point*. 
The error term in Taylors formula, when we represent a function with a finite number of polynomial elements is given by:

.. math::
        
        R_n(x)=f(x)-P_n(x)=\frac{f^{(n+1)}(\eta)}{(n+1)!}(x-c)^{n+1}\nonumber
        

.. _Eq:eq:taylor:error:

.. math::

    \tag{4}
    =\frac{1}{n!}\int_c^x(x-\tau)^{n}f^{(n+1)}(\tau)d\tau,\
        

for some :math:`\eta` in the domain :math:`[x,c]`.
If we want to calculate 
:math:`\sin x` to a precision lower than a specified value we can do it as follows:

.. code-block:: python

    import numpy as np
    
    # Sinus implementation using the Maclaurin Serie
    # By setting a value for eps this value will be used
    # if not provided
    def my_sin(x,eps=1e-16):
        f = power = x
        x2 = x*x
        sign = 1
        i=0
        while(power>=eps):
            sign = - sign
            power *= x2/(2*i+2)/(2*i+3)
            f += sign*power
            i += 1
        print('No function evaluations: ', i)
        return f
    
    x=0.8
    eps = 1e-9
    print(my_sin(x,eps), 'error = ', np.sin(x)-my_sin(x,eps))

This implementation needs some explanation:

* The error term is given in equation :ref:`(4) <Eq:eq:taylor:error>`, in this case it reduces to :math:`R_{2n}=(-1)^{n+1}x^{2n+1}\cos\eta/(2n+1)!` for terms up to :math:`k=n-1` in equation :ref:`(3) <Eq:sin>`. Since we do not know
  where to evaluate :math:`\eta` we just replace :math:`\cos\eta` with one (since :math:`\cos\eta\leq1`). We then add the higher order terms and check if the error term is low enough, since we add the error term 
  to the function evaluation our estimate will always be better than the specified accuracy.

* We evaluate the polynomials in the Taylor series by using the previous values too avoid too many multiplications within the loop, we do this by using the following identity:

.. math::
          
          \sin x=\sum_{k=0}^{\infty} (-1)^nt_n, \text{ where: } t_n\equiv\frac{x^{2n+1}}{(2n+1)!}, \text{ hence :}\nonumber
        

.. math::
          
          t_{n+1}=\frac{x^{2(n+1)+1}}{(2(n+1)+1)!}=\frac{x^{2n+1}x^2}{(2n+1)! (2n+2)(2n+3)}\nonumber
        

.. _Eq:_auto1:

.. math::

    \tag{5}
    =t_n\frac{x^2}{(2n+2)(2n+3)}
        
        

Evaluation of polynomials
-------------------------
How to evaluate a polynomial of the type: :math:`p_n(x)=a_0+a_1x+a_2x^2+\cdots+a_nx^n`? We already saw a hint in the previous section that it can be done in different ways. One way is simply to 
do:

.. code-block:: python

    pol = a[0]
    for i in range(1,n+1):
    	pol = pol + a[i]*x**i

Note that there are :math:`n` additions, whereas there are :math:`1 + 2 +3+\cdots+n=n(n+1)/2` multiplications for all the iterations. Instead of evaluating the powers all over in 
each loop, we can use the previous calculation to save the number of multiplications:

.. code-block:: python

    pol = a[0] + a[1]*x
    power = x
    for i in range(2,n+1):
    	power  = power*x
    	pol    = pol + a[i]*power

In this case there are still :math:`n` additions, but now there are :math:`2n-1` multiplications. For :math:`n=15`, this amounts to 120 for the first, and 29 for the second method. 
Polynomials can also be evaluated using *nested multiplication*:

.. math::
        
        p_1  = a_0+a_1x\nonumber
        

.. math::
          
        p_2  = a_0+a_1x+a_2x^2=a_0+x(a_1+a_2x)\nonumber
        

.. math::
          
        p_3  = a_0+a_1x+a_2x^2+a_3x^3=a_0+x(a_1+x(a_2+a_3x))\nonumber
        

.. _Eq:_auto2:

.. math::

    \tag{6}
    \vdots
        
           

and so on. This can be implemented as:

.. code-block:: python

    pol = a[n]
    for i in range(n-1,1,-1):
    	pol  = a[i] + pol*x

In this case we only have :math:`n` multiplications. So if you know beforehand exactly how many terms is needed to calculate the series, this method would be the preferred method, and is implemented in NumPy as `polyval <https://docs.scipy.org/doc/numpy/reference/generated/numpy.polyval.html#r138ee7027ddf-1>`__. 

.. _ch:taylor:der:

Calculating Derivatives of Functions
====================================

index{forward difference}

The derivative of a function can be calculated using the definition from calculus:

.. _Eq:eq:taylor:der1:

.. math::

    \tag{7}
    f^\prime(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}\simeq \frac{f(x+h)-f(x)}{h}.\
          

Not that :math:`h` can be both positive and negative, if :math:`h` is positive equation :ref:`(7) <Eq:eq:taylor:der1>` is termed *forward difference*, because we use the function value on the right (:math:`f(x+|h|)`). If on the other hand :math:`h` is negative equation :ref:`(7) <Eq:eq:taylor:der1>` is termed *backward difference*, because we use the value to the left (:math:`f(x-|h|)`). (:math:`|h|` is the absolute value of :math:`h`).
In the computer we cannot take the limit, :math:`h\to 0`, a natural question is then: What value to use for :math:`h`? 
In figure :ref:`fig:taylor:df`, we have evaluated the numerical derivative of :math:`\sin x`, using the formula in equation :ref:`(7) <Eq:eq:taylor:der1>` for different step sizes :math:`h`. 

.. _fig:taylor:df:

.. figure:: df.png
   :width: 600

   *Error in the numerical derivative of :math:`\sin x` at :math:`x=0.2` for different step size*

We clearly see that the error depends on the step size, but there is a minimum; choosing a step size too large give a poor estimate and choosing a too low step size give an 
even worse estimate. The explanation for this behavior is two competing effects: *mathematical approximation* and *round off errors*. Let us consider approximation or truncation error
first. By using the Taylor expansion in equation :ref:`(2) <Eq:eq:taylor:taylor>` and expand about :math:`x` and the error formula :ref:`(4) <Eq:eq:taylor:error>`, we get:

.. math::
        
        f(x+h)=f(x)+f^\prime(x)h + \frac{h^2}{2}f^{\prime\prime}(\eta)\text{ , hence:}\nonumber
        

.. _Eq:eq:taylor:derr:

.. math::

    \tag{8}
    f^\prime(x)=\frac{f(x+h)-f(x)}{h}-\frac{h}{2}f^{\prime\prime}(\eta),\
        

for some :math:`\eta` in :math:`[x,x+h]`. Thus the error to our approximation is :math:`hf^{\prime\prime}(\eta)/2`, if we reduce the step size by a factor of 10 the error is reduced by a factor of 10. 
Inspecting the graph, we clearly see that this is correct as the step size decreases from :math:`10^{-1}` to :math:`10^{-8}`. When the step size decreases more, there is an increase in the error. This
is due to round off errors, and can be understood by looking into how numbers are stored in a computer.  

Big :math:`\mathcal{O}` notation
--------------------------------
`example <https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/>`__

Round off Errors
----------------
In a computer a floating point number,$x$, is represented as:

.. _Eq:_auto3:

.. math::

    \tag{9}
    x=\pm q2^m.
        
        

Most computers are 64-bits, then one bit is reserved for the sign, 52 for the fraction (:math:`q`) and 11 for
the exponent (:math:`m`)  (for a graphic illustration see `Wikipedia <https://en.wikipedia.org/wiki/Double-precision_floating-point_format>`__).
what is the largest *floating point* number the computer can represent? 
Since :math:`m` contains 11 bits, :math:`m` can have the maximal value :math:`m=2^{11}=1024`, and then the larges value is close to :math:`2^{1024}\simeq 10^{308}`.
If you enter ``print(10.1*10**(308))`` in Python the answer will be ``Inf``. If you enter ``print(10*10**(308))``, Python will give an answer. This is because 
the number :math:`10.1\cdot10^{308}` is floating point number, whereas :math:`10^{309}` is an *integer*, and Python does something clever when it comes to representing integers. 
Python has a third numeric type called long int, which can use the available memory to represent an integer. 

:math:`10^{308}` is the largest number, but what is the highest precision we can use, or how many decimal places can we use for a floating point number? 
Since there are 52 bits for the fraction, there are :math:`1/2^{52}\simeq10^{-16}` decimal places. As an example
the value of :math:`\pi` is :math:`3.14159265358979323846264338\ldots`, but in Python it can only be represented by 16 digits: :math:`3.141592653589793`. In principle 
it does not sound so bad to have an answer accurate to 16 digits, and it is much better than most experimental results. 
So what is the problem? One problem that you should be aware of is that round off errors can be a serious problem when we subtract two numbers that 
are very close to one another. If we implement the following program in Python:

.. code-block:: python

    h=1e-16
    x = 2.1 + h
    y = 2.1 - h
    print((x-y)/h)

we expect to get the answer 2, but instead we get zero. By changing :math:`h` to a higher value, the answer will get closer to 2. 

Armed with this knowledge of round off errors, we can continue to analyze
the result in figure :ref:`fig:taylor:df`.
The round off error when we represent a floating point number :math:`x` in the 
machine will be of the order :math:`x/10^{16}` (*not* :math:`10^{-16}`). In general, when we evaluate a function the error will be of the order 
:math:`\epsilon|f(x)|`, where :math:`\epsilon\sim10^{-16}`. Thus equation :ref:`(8) <Eq:eq:taylor:derr>` is modified in the following way when we take into account the round off errors:

.. _Eq:eq:taylor:derr2:

.. math::

    \tag{10}
    f^\prime(x)=\frac{f(x+h)-f(x)}{h}\pm\frac{2\epsilon|f(x)|}{h}-\frac{h}{2}f^{\prime\prime}(\eta),\
        

we do not know the sign of the round off error, so the total error :math:`R_2` is:

.. _Eq:eq:taylor:derr3:

.. math::

    \tag{11}
    R_2=\frac{2\epsilon|f(x)|}{h}+\frac{h}{2}|f^{\prime\prime}(\eta)|.\
        

We have put absolute values around the function and its derivative to get the maximal error, it might be the case that the round off error cancel part of the 
truncation error. However, the round off error is random in nature and will change from machine to machine, and each time we run the program. 
Note that the round off error increases when :math:`h` decreases, and the approximation error decreases when :math:`h` decreases. This is exactly what we see in the figure above. We can find the 
best step size, by differentiating :math:`R_2` and put it equal to zero:

.. math::
        
        \frac{dR_2}{dh}=-\frac{2\epsilon|f(x)|}{h^2}+\frac{1}{2}f^{\prime\prime}(\eta)=0\nonumber
        

.. _Eq:eq:taylor:derr4:

.. math::

    \tag{12}
    h=2\sqrt{\epsilon\left|\frac{f(x)}{f^{\prime\prime}(\eta)}\right|}\simeq 2\cdot10^{-8},\
        

In the last equation we have assumed that :math:`f(x)` and its derivative is :math:` |nbsp| 1`. This step size corresponds to an error of order :math:`R_2\sim10^{-8}`. 
Inspecting 
the result in figure :ref:`fig:taylor:df`.
we see that the minimum is located at :math:`h\sim10^{-8}`.      

Higher Order Derivatives
========================
There are more ways to calculate the derivative of a function, than the formula given in equation :ref:`(8) <Eq:eq:taylor:derr>`. Different formulas can be
derived by using Taylors formula in :ref:`(2) <Eq:eq:taylor:taylor>`, usually one expands about :math:`x\pm h`:

.. math::
        
        f(x+h)=f(x)+f^\prime(x)h + \frac{h^2}{2}f^{\prime\prime}(x)+ \frac{h^3}{3!}f^{(3)}(x)+ \frac{h^4}{4!}f^{(4)}(x)+\cdots\nonumber
        

.. _Eq:_auto4:

.. math::

    \tag{13}
    f(x-h)=f(x)-f^\prime(x)h + \frac{h^2}{2}f^{\prime\prime}(x)- \frac{h^3}{3!}f^{(3)}(x)+ \frac{h^4}{4!}f^{(3)}(x)-\cdots.
        
        

If we add these two equations, we get an expression for the second derivative, because the first derivative cancels out. But we also observe that if we subtract these two equations we get 
an expression for the first derivative that is accurate to a higher order than the formula in equation :ref:`(7) <Eq:eq:taylor:der1>`, hence:

.. _Eq:eq:taylor:der2:

.. math::

    \tag{14}
    f^\prime(x)=\frac{f(x+h)-f(x-h)}{2h} -\frac{h^2}{6}f^{(3)}(\eta),\
        

.. _Eq:eq:taylor:2der:

.. math::

    \tag{15}
    f^{\prime\prime}(x) = \frac{f(x+h)+f(x-h)-2f(x)}{h^2}+ \frac{h^2}{12}f^{(4)}(\eta)\,
        

for some :math:`\eta` in :math:`[x,x+h]`. In figure :ref:`fig:taylor:df2`, we have plotted equation :ref:`(8) <Eq:eq:taylor:derr>`, :ref:`(14) <Eq:eq:taylor:der2>`, and :ref:`(15) <Eq:eq:taylor:2der>` for 
different step sizes. The derivative in equation :ref:`(14) <Eq:eq:taylor:der2>`, gives a higher accuracy than equation :ref:`(8) <Eq:eq:taylor:derr>` for a larger step size,
as can bee seen in figure :ref:`fig:taylor:df2`.

.. _fig:taylor:df2:

.. figure:: df2.png
   :width: 600

   *Error in the numerical derivative and second derivative of :math:`\sin x` at :math:`x=0.2` for different step size*

We can perform a similar error analysis as we did before, and then we find for equation :ref:`(14) <Eq:eq:taylor:der2>` and :ref:`(15) <Eq:eq:taylor:2der>` that the total
numerical error is:

.. _Eq:eq:taylor:derr3b:

.. math::

    \tag{16}
    R_3=\frac{\epsilon|f(x)|}{h}+\frac{h^2}{6}f^{(3)}(\eta),\
        

.. _Eq:eq:taylor:derr4b:

.. math::

    \tag{17}
    R_4=\frac{4\epsilon|f(x)|}{h^2}+\frac{h^2}{12}f^{(4)}(\eta),\
        

respectively. Differentiating these two equations with respect to :math:`h`, and set the equations equal to zero, we find an optimal step size of
:math:`h\sim10^{-5}` for equation :ref:`(16) <Eq:eq:taylor:derr3b>`, which gives an error of :math:`R_2\sim 10^{-16}/10^{-5}+(10^{-5})^2/6\simeq10^{-10}`, and :math:`h\sim10^{-4}` for equation
:ref:`(17) <Eq:eq:taylor:derr4b>`, which gives an error of :math:`R_4\sim 4\cdot10^{-16}/(10^{-4})^2+(10^{-4})^2/12\simeq10^{-8}`. Note that we get the surprising result for the first order 
derivative in equation :ref:`(14) <Eq:eq:taylor:der2>`, that a higher step size gives a more accurate result. 

