Solving systems of equations are one of the most common tasks that we use computers for within modeling. A typical task is that we have have a model that contains a set of unknown parameters which we want to determine. To determine these parameters we need to solve a set of equations. In many cases these equations are nonlinear, but often a nonlinear problem is solved
*by linearize* the nonlinear equations, and thereby reducing it to a sequence of linear algebra problems. Thus the topic of solving linear systems of equations have been extensively studied, and sophisticated linear equation solving packages have been developed. Python uses functions from the "LAPACK":"https://en.wikipedia.org/wiki/LAPACK" library. In this course we will only cover the theory behind numerical linear algebra superficially, and the main purpose is to shed some light on some of the challenges one might encounter solving linear systems.

After covering some basics of numerical linear algebra, we will shift focus to nonlinear equations. Contrary to linear equations, you will most likely find that the functions available in various Python library will *not* cover your needs and in many cases fail to give you the correct solution. The reason for this is that the solution of a nonlinear equation is greatly dependent on the starting point, and a combination of various techniques must be used.  

======= Solving linear equations =======
There are a number of excellent books covering this topic, see e.g. cite{press2007,trefethen1997,stoer2013,strang2019}. The purpose of this section is to get insight into some of the challenges when solving linear equations numerically, and for you to understand when it is stated in the NumPy documentation that the standard linear solver: "`solve`":"https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html" function uses *LU-decomposition* and *partial pivoting*. 

In most of the examples covered in this course we will encounter problems where we have a set of *linearly independent* equations and one equation for each unknown. For these type of problems there are a number of methods that can be used, and they will find a solution in a finite number of steps. If a solution cannot be found it is usually because the equations are not linearly independent, and our formulation of the physical problem is wrong.

Assume that we would like to solve the following set of equations:
!bt
\begin{align}
2x_0+x_1+x_2+3x_3&=1,label{eq:nlin:la} \\
x_0+x_1+3x_2+x_3&=-3,label{eq:nlin:lb} \\
x_0+4x_1+x_2+x_3&=2,label{eq:nlin:lc} \\
x_0+x_1+x_2+x_3&=1.label{eq:nlin:ld} 
\end{align}
!et
These equations can be written in matrix form as:
!bt
\begin{equation}
\mathbf{A\cdot x}=\mathbf{b},
label{eq:nlin:mat}
\end{equation}
!et
where:
!bt
\begin{equation}
\mathbf{A}\equiv\begin{pmatrix}
2&1&1&3\\
1&1&3&1\\
1&4&1&1\\
1&1&2&2
\end{pmatrix}
\qquad
\mathbf{b}\equiv
\begin{pmatrix}
1\\-3\\2\\1
\end{pmatrix}
\qquad
\mathbf{x}\equiv
\begin{pmatrix}
x_0\\x_1\\x_2\\x_3
\end{pmatrix}.
label{eq:nlin:matA}
\end{equation}
!et
You can easily verify that $x_0=-4, x_1=1, x_2=-1, x_3= 3$ is the solution to the above equations by direct substitution. If we were to replace one of the above equations with a linear combination of any of the other equations, e.g. replace equation (ref{eq:nlin:ld}) with $3x_0+2x_1+4x_2+4x_3=-2$, there would be no solution. This can be checked by calculating the determinant of the matrix $\mathbf{A}$, if $\det \mathbf{A}=0 $,  
What is the difficulty in solving these equations? Clearly if none of the equations are linearly dependent, and we have $N$ independent linear equations, it should be straight forward to solve them? Two major numerical problems are i) even if the equations are not exact linear combinations of each other, they could be very close, and as the numerical algorithm progresses they could at some stage become linearly dependent due to roundoff errors. ii) roundoff errors may accumulate if the number of equations are large cite{press2007}.

===== Gauss-Jordan elimination =====
Let us continue the discussion by consider Gauss-Jordan elimination, which is a *direct* method. A direct method uses a final set of operations to obtain a solution. According to cite{press2007} Gauss-Jordan elimination is the method of choice if we want to find the inverse of $\mathbf{A}$. However, it is slow when it comes to calculate the solution of equation
(ref{eq:nlin:mat}). Even if speed and memory use is not an issue, it is also not advised to first find the inverse, $\mathbf{A}^{-1}$, of $\mathbf{A}$, then multiply it with $\mathbf{b}$ to obtain the solution, due to roundoff errors (Roundoff errors occur whenever we subtract to numbers that are very close to each other). To simplify our notation, we write equation (ref{eq:nlin:matA}) as:
!bt
\begin{equation}
\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
1&1&3&1&-3\\
1&4&1&1&2\\
1&1&2&2&1
\end{array}
\right).
\end{equation}
!et
The numbers to the left of the vertical dash is the matrix $\mathbf{A}$, and to the right is the vector $\mathbf{b}$. The Gauss-Jordan elimination procedure proceeds by doing the same operation on the right and left side of the dash, and the goal is to get only zeros on the lower triangular part of the matrix. This is achieved by multiplying rows with the same (nonzero) number, swapping rows, adding a multiple of a row to another:
!bt
\begin{align}
&\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
1&1&3&1&-3\\
1&4&1&1&2\\
1&1&2&2&1
\end{array}
\right)\to
\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
0&1/2&5/2&-1/2&-7/2\\
0&7/2&1/2&-1/2&3/2\\
0&1/2&3/2&1/2&1/2
\end{array}
\right)\to\label{eq:nlin:gj1}\\
&\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
0&1/2&5/2&-1/2&-7/2\\
0&0&-17&3&26\\
0&0&1&-1&4
\end{array}
\right)
\to
\left(
\begin{array}{cccc|c}
2&1&1&3&1\\
0&1/2&5/2&-1/2&-7/2\\
0&0&-17&3&26\\
0&0&0&14/17&42/17
\end{array}
\right)\no
\end{align}
!et
The operations done are: ($1\to2$) multiply first row with $-1/2$ and add to second, third and the fourth row, ($2\to 3$) multiply second row with $-7$, and add to third row, multiply second row with $-1$ and add to fourth row, ($3\to4$) multiply third row with $-1/17$ and add to fourth row. These operations can easily be coded into Python:
@@@CODE src-nlin/nlin_sym.py fromto: A = np.array@# Back
Notice that the final matrix has only zeros beyond the diagonal, such a matrix is called *upper triangular*. We still have not found the final solution, but from an upper triangular (or lower triangular) matrix it is trivial to determine the solution. The last row immediately gives us $14/17z=42/17$ or $z=3$, now we have the solution for z and the next row gives: $-17y+3z=26$ or $y=(26-3\cdot3)/(-17)=-1$, and so on. In a more general form, we can write our solution of the matrix $\mathbf{A}$ after making it upper triangular as:
!bt
\begin{equation}
\begin{pmatrix}
a^\prime_{0,0}&a^\prime_{0,1}&a^\prime_{0,2}&a^\prime_{0,3}\\
0&a^\prime_{1,1}&a^\prime_{1,2}&a^\prime_{1,3}\\
0&0&a^\prime_{2,2}&a^\prime_{2,3}\\
0&0&0&a^\prime_{3,3}
\end{pmatrix}
\cdot
\begin{pmatrix}
x_0\\
x_1\\
x_2\\
x_3
\end{pmatrix}
=
\begin{pmatrix}
b^\prime_{0}\\
b^\prime_{1}\\
b^\prime_{2}\\
b^\prime_{3}
\end{pmatrix}
label{eq:nlin:back}
\end{equation}
!et
The backsubstitution can then be written formally as:
!bt
\begin{equation}
x_i=\frac{1}{a^\prime_{ii}}\left[b_i^\prime-\sum_{j=i+1}^{N-1}a^\prime_{ij}x_j\right],\quad i=N-1,N-2,\ldots,0
label{eq:nlin:back2}
\end{equation}
!et
The backsubstitution can now easily be implemented in Python as:
@@@CODE src-nlin/nlin_sym.py fromto: # Back@# Backsubstitution - for loop
Notice that in the Python implementation, we have used vector operations instead of for loops. This makes the code more efficient, but it could also be implemented with for loops: 
@@@CODE src-nlin/nlin_sym.py fromto: # Backsubstitution - for loop@print
There are at least two things to notice with our implementation:
* Matrix and vector notation makes the code more compact and efficient. In order to understand the implementation it is advised to put $i=1, 2, 3, 4$, and then execute the statements in the Gauss-Jordan elimination and compare with equation (ref{eq:nlin:gj1}).
* The implementation of the Gauss-Jordan elimination is not robust, in particular one could easily imagine cases where one of the leading coefficients turned out as zero, and the routine would fail when we divide by `A[i-1,i-1]`. By simply changing equation (ref{eq:nlin:lb}) to $2x_0+x_1+3x_2+x_3=-3$, when doing the first Gauss-Jordan elimination, both $x_0$ and $x_1$ would be cancelled. In the next iteration we try to divide next equation by the leading coefficient of $x_1$, which is zero, and the whole procedure fails.

The solution to the last problem is solved by what is called *pivoting*. The element that we divide on is called the *pivot element*. It actually turns out that even if we do Gauss-Jordan elimination *without* encountering a zero pivot element, the Gauss-Jordan procedure is numerically unstable in the presence of roundoff errors cite{press2007}. There are two versions of pivoting, *full pivoting* and *partial pivoting*. In partial pivoting we only interchange rows, while in full pivoting we also interchange rows and columns. Partial pivoting is much easier 

, then multiply first row with $-1$ and add to 
There , there is a good explanation of why there can be 
In the case of a linearly independent matrix $A$, of a reasonable size, we can use Gaussian elimination.  

======= Solving nonlinear equations =======
The purpose of this section is to introduce a handful of techniques for solving a nonlinear equation. In many cases a combination of methods must be used, and the algorithm must be adopted to your specific problem. 
